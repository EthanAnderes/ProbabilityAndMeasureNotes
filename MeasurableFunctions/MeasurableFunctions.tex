
%-------------------------------'
%---------section  ---------------'
%-------------------------------'
\section{Measurable functions}
%-------------------------------'
%---------section  ---------------'
%-------------------------------'

%\subsection{Basic theory}

\begin{definition}[{\bf Measurable functions}]
If $(\Omega_1, \mathcal F_1)$ and $(\Omega_2,\mathcal F_2)$ are measurable spaces then $f\colon \Omega_1 \rightarrow \Omega_2$ is said to be {measurable between $\mathcal F_1$ and $\mathcal F_2$} (written $f\mcirc \mathcal F_1/\mathcal F_2$) if and only if
\[ f^{-1}(A)\in \mathcal F_1,\quad\forall A\in\mathcal F_2\]
where $ f^{-1}(A):=\{w\in\Omega_1: f(w)\in A \}$.
\end{definition}


\begin{theorem}[{\bf Basic facts about pull backs}]
\label{thm pull back basic facts}
Let $f\colon \Omega_1 \rightarrow \Omega_2$. Let $A, A_1, A_2, \ldots \subset \Omega_2$. Then
\begin{itemize}
\item $f^{-1}(\Omega_2) = \Omega_1$
\item $f^{-1}(\varnothing) = \varnothing$
\item $f^{-1}(\Omega_2 - A) = \Omega_1 - f^{-1}(A)$
\item $f^{-1}(\cup_i A_i) =  \cup_i  f^{-1}(A_i)$
\item $f^{-1}(\cap_i A_i) = \cap_i  f^{-1}(A_i)$.
\end{itemize}
\end{theorem}
\begin{proof} These are very easy to check. For example  to see why $f^{-1}(\cup_i A_i) \subset  \cup_i  f^{-1}(A_i)$ notice that
\begin{align*}
\omega \in f^{-1}(\cup_i A_i)
&\Longrightarrow f(\omega) \in \cup_i A_i \\
&\Longrightarrow \text{$f(\omega) \in A_i$ for some $i$ }\\
&\Longrightarrow \text{$\omega \in f^{-1}(A_i)$ for some $i$ }\\
&\Longrightarrow \omega \in \cup_i f^{-1}(A_i).
\end{align*}
The other arguments are exactly similar.
\end{proof}


\begin{theorem}[{\bf Generators are enough}]
\label{thm: GaE}
Let $(\Omega_1, \mathcal F_1)$ and $(\Omega_2,\mathcal F_2)$ be  measurable spaces where $\mathcal F_2$ is generated by some class $\mathcal A\subset 2^{\Omega_2}$ (i.e. $\mathcal F_2=\sigma\langle \mathcal A\rangle$). If $f:\Omega_1\rightarrow \Omega_2$ then
\[ f \mcirc \mathcal F_1/\mathcal F_2 \Longleftrightarrow f^{-1}(A)\in\mathcal F_1,\,\, \forall A\in\mathcal A.\]
\end{theorem}



\begin{corollary}[{\bf Monotone real maps are measurable}]
Any monotone map $f\colon \Bbb R\rightarrow \Bbb R$ is measurable $\mathcal B^{\Bbb R}/\mathcal B^{\Bbb R}$.
\end{corollary}

\begin{corollary}[{\bf Continuous real maps are measurable}]
Any continuous map $f\colon \Bbb R^d\rightarrow \Bbb R^k$ is measurable $\mathcal B^{\Bbb R^d}/\mathcal B^{\Bbb R^k}$.
\end{corollary}




\begin{theorem}[{\bf Composition of $\mcirc$ functions is $\mcirc$}]
\label{thm: composition of measurable}
Let $(\Omega_1,\mathcal F_1)$, $(\Omega_2,\mathcal F_2)$ and $(\Omega_3,\mathcal F_3)$ be measurable spaces. Suppose $f$ and $g$ are functions sending $\Omega_1\overset{f} \longmapsto \Omega_2 \overset{g}\longmapsto \Omega_3$. If $f\mcirc \mathcal F_1/\mathcal F_2$ and $g\mcirc  \mathcal F_2/\mathcal F_3$ then $g\circ f \mcirc\mathcal F_1/\mathcal F_3$.
\end{theorem}


\begin{corollary}[{\bf Just check that each coordinate is $\mcirc$}]
\label{coordM}
Let $(\Omega,\mathcal F)$ be a measurable space and $f:\Omega\rightarrow \Bbb R^d$. Let $f=(f_1,\ldots, f_d)$ decompose $f$ into the coordinate functions (so that $f_k:\Omega\rightarrow \Bbb R$). Then
\[ f\mcirc \mathcal F/\mathcal B^{\Bbb R^d}\Longleftrightarrow  f_k\mcirc \mathcal F/\mathcal B^{\Bbb R}, \text{ for each $k$.} \]
\end{corollary}



\begin{theorem}[{\bf Scissors and paste}]
Let $(\Omega_1, \mathcal F_1)$ and $(\Omega_2,\mathcal F_2)$ be measurable spaces and $f\colon \Omega_1 \rightarrow \Omega_2$.  In addition, suppose there exists $\mathcal F_1$-sets $A_1,A_2,\ldots$ such that $\Omega_1 = \cup_{k=1}^\infty A_k$. Then
\[ f\mcirc \mathcal F_1/\mathcal F_2 \Longleftrightarrow  f\bigr|_{A_k} \mcirc (\mathcal F_1\!\cap\! A_k)/\mathcal F_2, \text{ for each $k$.} \]
\end{theorem}

\begin{corollary}[{\bf Piecewise-continuous real maps are measurable}]
Any map $f\colon \Bbb R^d\rightarrow \Bbb R^k$ which is piecewise continuous on each piece of a  countable-measurable-partition of $\Bbb R^d$  is measurable $\mathcal B^{\Bbb R^d}/\mathcal B^{\Bbb R^k}$.
\end{corollary}


\begin{theorem}[{\bf Metric-continuous functions  are measurable}]
Suppose $\Omega_1$ and $\Omega_2$ are metric spaces and $f$ is a function mapping $\Omega_1$ into $\Omega_2$.
If there exists $\mathcal B^{\Omega_1}$ sets $A_1,A_2,\ldots$ such that $\Omega_1=\cup_{k=1}^\infty A_k$ and $f\bigr|_{A_k}$ is continuous (with respect to the induced metrics) on each $A_k$ then $f$ is measurable $\mathcal B^{\Omega_1}/\mathcal B^{\Omega_2}$.
\end{theorem}


\begin{theorem}[{\bf Just check Borel $\mcirc$ on the range}]
Let $(\Omega_1, \mathcal F_1)$ be a measurable space and $\Omega_2$ be a metric space.
Suppose $f$ is a function which maps $\Omega_1$ into $\Omega_{2}^o\subset \Omega_2$. Then
\[ f\mcirc \mathcal F_1/\mathcal B^{\Omega_2^o} \Longleftrightarrow f\mcirc \mathcal F_1/\mathcal B^{\Omega_2}  \]
where the metric used to define $\mathcal B^{\Omega_2^o}$ is the one induced by the metric on $\Omega_2$.
\end{theorem}

\begin{definition}[{\bf Nomenclature for the extended reals}]\label{nonmen}
$\vphantom{asdf}$
\begin{itemize}
\item $\mathcal B$ is shorthand notation for $\mathcal B^{\bar{\Bbb R}}$.
\item If $(\Omega, \mathcal F)$  is a measurable space and $f:\Omega \rightarrow \bar{\Bbb R}$ we use the nomenclature  `$\mcirc \mathcal F$' or just `measurable $\mathcal F$' as short hand for $\mcirc \mathcal F/\mathcal B$.
\item We say that a function $f$ is `Borel measurable' or just `measurable' if $f:\Bbb R^d \rightarrow \bar{\Bbb R}$ and $f$ is $\mcirc {\mathcal B^{\Bbb R^d}}/\mathcal B$.
\item We say that a function $f$ is `Lebesque measurable'  if $f:\Bbb R^d \rightarrow \bar{\Bbb R}$ and $f$ is $\mcirc \overline{\mathcal B^{\Bbb R^d}}/\mathcal B$.
\end{itemize}
\end{definition}

\begin{definition}[{\bf Defining algebraic operations with $\infty$}]
$\vphantom{asdf}$
\begin{itemize}
\item $\infty + c := \infty$  for any $-\infty< c\leq \infty$.
\item $\infty\cdot 0 =0\cdot\infty:=0$.
\item $\infty\cdot \infty := \infty$.
\item $\frac{c}{\infty}:=0$ when $c\in \Bbb R$.
\item $\frac{c}{0}$, $\frac{\pm\infty}{\pm\infty}$,  $\infty - \infty$ and $-\infty + \infty$ are not defined.
\end{itemize}
\end{definition}

\begin{theorem}[{\bf Closure theorem for $\mcirc$ functions}]
If $(\Omega, \mathcal F)$ is a measurable space then
\begin{enumerate}
\item If $f$ and $g$ are  $\mcirc \mathcal F$ functions then $cf$ (c is a constant), $f+g$, $fg$, $f/g$, $f\vee g$, $f\wedge g$, $f^+$, $f^-$ are each $\mcirc \mathcal F$, provided the composite function are defined at every $w\in\Omega$.
\item If $f_1, f_2,\ldots $  $\mcirc \mathcal F$ functions then $\sup_n f_n$, $\inf_n f_n$, $\limsup_n f_n$, $\liminf_n f_n$ are each $\mcirc \mathcal F$.
\end{enumerate}
\end{theorem}



\begin{definition}[{\bf Simple functions}]
Let $(\Omega, \mathcal F)$ denote a measurable space. Then
any function $f\colon \Omega \rightarrow \bar{\Bbb R}$ which is $\mcirc \mathcal F$ and has a finite range is called a {simple function}.
\end{definition}


\begin{definition}[{\bf Characterization of simple functions}]
Let $(\Omega, \mathcal F)$ denote a measurable space and suppose  $f\colon \Omega \rightarrow \bar{\Bbb R}$. Then $f$ is a simple function if and only if there exists a finite partition of $\Omega$ into disjoint $\mathcal F$-sets $A_1,\ldots, A_n$
and a finite list of extended real numbers $c_1,\ldots, c_n$
 such that $f=\sum_{k=1}^n c_k I_{A_k}$.
\end{definition}





\begin{definition}[{\bf $\mathscr N_s$ and $\mathscr N$}]
Let $(\Omega, \mathcal F)$ denote a measurable space. Then
\begin{itemize}
\item $\mathscr N_s$ denotes the set of non-negative simple functions \mbox{$f:\Omega\rightarrow \bR$}.
\item $\mathscr N$ denotes the set of non-negative functions $f:\Omega\rightarrow \bR$ which are  $\mcirc \mathcal F$.
\end{itemize}
\end{definition}



\begin{theorem}[{\bf The structure theorem}]
\label{thm: structure thm}
Let $(\Omega, \mathcal F)$ be a measurable space and suppose $f\colon \Omega\rightarrow \bar{\Bbb R}$ is $\mcirc \mathcal F$. Then
\begin{enumerate}
\item There exists bounded simple functions $f_1, f_2,\ldots $ such that $\lim_n f_n(w)= f(w)$ for each $w\in \Omega$.
\item If, in addition,  $f\in\mathscr N$  then there exists bounded $f_1, f_2, \ldots \in\mathscr N_s$ such that $ f_n(w)\uparrow f(w)$ for each $w\in \Omega$.
\end{enumerate}
\end{theorem}


\begin{exercise} Show that Corollary \ref{coordM} holds for functions mapping into $\bar{\Bbb R}^d$.
\end{exercise}


\begin{exercise}
Let $(\Omega, \mathcal F)$ be a measure space and let $f_0, f_1,\ldots$ be an infinite sequence of $\mathcal F$-measurable functions of $\Omega$. Show that the radius $R$ of convergence of the random power series $\sum_{k=0}^\infty f_k x^k$ is an $\mathcal F$-measurable function of $\Omega$.
\end{exercise}


\begin{exercise}
Give an example of two measurable spaces $(\Omega_1, \mathcal F_1)$,  $(\Omega_2,\mathcal F_2)$, a $\mcirc \mathcal F_1/\mathcal F_2$ mapping $f:\Omega_1\rightarrow \Omega_2$,  and an event $B\in\mathcal F_1$ such that $f(B)\notin \mathcal F_2$.
\end{exercise}



%%%%%%%%%%%%%%%%%
%
%  random variable subsection
%
%%%%%%%%%%%%%%%%%%%%%
\subsection{Application: Random variables and distribution functions }



\begin{theorem}[{\bf Induced measures}]
Let $(\Omega_1, \mathcal F_1)$ and $(\Omega_2,\mathcal F_2)$ be two measurable spaces. Suppose $f:\Omega_1\rightarrow \Omega_2$ is $\mcirc \mathcal F_1/\mathcal F_2$ and $\mu$ is a measure on $\Omega_1$. Then the set function defined by
\[ \mu f^{-1}(B):=\mu(f^{-1}(B)),\quad\text{for all $B\in\mathcal F_2$}\]
 is a measure on $(\Omega_2,\mathcal F_2)$ and is called the {\bf induced measure} on $(\Omega_2,\mathcal F_2)$. Moreover,
 \begin{align*}
 &\bullet\text{$\mu$ is a probability measure} \Longrightarrow  \text{$\mu f^{-1}$ is a probability measure;}\\
 &\bullet\text{$\mu$ is a finite measure} \Longrightarrow  \text{$\mu f^{-1}$ is a finite measure;} \\
 &\bullet\text{$\mu$ is a $\sigma$-finite measure} \not\Longrightarrow  \text{$\mu f^{-1}$ is a $\sigma$-finite measure.}
 \end{align*}
\end{theorem}


Give example of where the induced measure is not $\sigma$-finite but the base measure is.



\begin{definition}[{\bf Random variable}]
Any function $X\colon \Omega \rightarrow \Bbb R$ which is measurable $\mathcal F/\mathcal B^{\Bbb R}$  is said to be a {\bf random variable}.
\end{definition}

Distribution function are useful for making random variables with the specified induced distribution.

\begin{definition}[{\bf Distribution function on $\Bbb R$}]
A function $F\colon\Bbb R\rightarrow\Bbb R$ if called a {\bf distribution function} if $F$ satisfies the following three requirements:
\begin{itemize}
\item $F$ is non-decreasing;
\item $F$ is right continuous;
\item $\lim_{x\rightarrow \infty} F(x)=1$ and $\lim_{x\rightarrow -\infty} F(x)=0$.
\end{itemize}
\end{definition}


\begin{theorem}[{\bf Df's determine $PX^{-1}$}]
If $F$ is a distribution function then there exists a random variable $X$ defined on some probability space $(\Omega, \mathcal F, P)$ such that
\[ P(X\leq x) = F(x)\text{ for all $x\in\Bbb R$}. \]
Moreover, the distribution of $X$ is uniquely determined by $F$.
\end{theorem}


\begin{theorem}[{\bf $F^{-1}(U)\sim X$}]
Let $X$ be a random variable and define $F(x):=P(X\leq x)$. Let $U$ be a random variable uniformly distributed over $(0,1)$.  Then $F$ is a distribution function and $F^{-1}(U)\sim X$ (i.e. $F^{-1}(U)$ and $X$ have the same induced distribution on $\Bbb R$) where
\begin{equation}
\label{inverse CDF definition}
F^{-1}(u):=\inf\{x\in\Bbb R\colon u\leq F(x)  \}.
\end{equation}
\end{theorem}


\begin{theorem}[{\bf  $F(X)\sim U$}]
Let $X$ be a random variable with distribution function $F(x)=P(X\leq x)$. Then
\[ P(F(X)\leq u)\leq u\text{ for all $0<u<1$.} \]
Moreover, $F$ is continuous if and only if $P(F(X)\leq u)= u$ for all $0<u<1$.
\end{theorem}




\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{$\sigma$-fields generated by functions}



The results here will be used often in the later text. We will use them for generating a product measure, for Fubini's theorem and for conditional expected value.





\begin{definition}[{\bf The $\sigma$-field generated by functions}]
\label{def: sig generated by funs}
Let $\mathcal I$ be an arbitrary index set.
Let $(\Omega_i, \mathcal F_i)$ be a collection of measurable spaces indexed by $i\in \mathcal I$.
Let $f_i:\Omega \rightarrow \Omega_i$ be a  collection of functions indexed by $i\in \mathcal I$. Then the {\bf $\sigma$-field generated by $\{f_i: i\in\mathcal I\}$} is defined as
\[ \sigma\langle  f_i, \mathcal F_i:i\in \mathcal I \rangle :=  \bigcap_{\shortstack{\text{\small $\mathcal F$ is a $\sigma$-field on $\Omega$}  \\
 \text{\small each $X_i$ is $\mcirc \mathcal F/\mathcal F_i$ }}}\mathcal F\]
 and corresponds to the smallest $\sigma$-field  on $\Omega$ which makes all the random variables $f_i$ measurable.
 \end{definition}


When $\mathcal F_i$ are clear from context we may, and do, write
\[
\text{
$\sigma\langle f_i\colon i\in\mathcal I \rangle$ as shorthand for $\sigma\langle f_i, \mathcal F_i\colon i\in\mathcal I \rangle$.
}
\]


\begin{theorem}[{\bf Pull back for one map}]
For a single function $f_1:\Omega \rightarrow \Omega_1$ where $(\Omega_1, \mathcal F_1)$ is a measurable space one has that $\sigma \langle f_1,\mathcal F_1\rangle = f^{-1}(\mathcal F_1):= \{f^{-1}(F)\colon F\in \mathcal F_1 \}$.
\end{theorem}
\begin{proof}
We immediately have that $f^{-1}(\mathcal F_1)\subset \sigma \langle f_1,\mathcal F_1\rangle$ since by definition $f$ is  $\mcirc \sigma \langle f_1,\mathcal F_1\rangle/\mathcal F_1$.
To show $\sigma \langle f_1,\mathcal F_1\rangle\subset f^{-1}(\mathcal F_1)$, all we need is to establish that $f^{-1}(\mathcal F_1)$ is a $\sigma$-field (since trivially $f$ is  $\mcirc f^{-1}(\mathcal F_1)/\mathcal F_1$). This is easily checked by the properties of pull-back sets given in Theorem \ref{thm pull back basic facts}.
\end{proof}


\begin{theorem}[{\bf Generators are enough}]
If, in addition to the assumptions presented in Definition \ref{def: sig generated by funs}, one has that each $\mathcal F_i = \sigma\langle \mathcal A_i \rangle$, then
\[\sigma\langle f_i, \mathcal F_i\colon i\in \mathcal I\rangle = \sigma \bigl\langle f_i^{-1}(A_i)\colon A_i\in \mathcal A_i, i\in \mathcal I \bigr\rangle. \]
\end{theorem}
\begin{proof}
The only interesting direction is to show $\sigma\langle f_i, \mathcal F_i\colon i\in \mathcal I\rangle \subset \sigma \bigl\langle f_i^{-1}(A_i)\colon A_i\in \mathcal A_i, i\in \mathcal I \bigr\rangle$. By ``good sets'' we just need to show that each $f_i$ is $\mcirc \sigma \bigl\langle f_i^{-1}(A_i)\colon A_i\in \mathcal A_i, i\in \mathcal I \bigr\rangle / \sigma\langle\mathcal A_i\rangle$. This follows immediately from Theorem \ref{thm: GaE} (generators are enough).
\end{proof}




\begin{definition}[{\bf The product $\sigma$-field}]
Let $\mathcal I$ be an arbitrary index set. Let $(\Omega_i, \mathcal F_i)$ be a collection of measurable spaces indexed by $i\in \mathcal I$. Define the product $\sigma$-field on $\prod_{i\in \mathcal I}\Omega_i$  to be
\[
\textstyle\bigotimes_{i\in\mathcal I} \mathcal F_i := \sigma\langle \pi_i,\mathcal F_i\colon i\in \mathcal I \rangle
\]
where $\pi_i\colon \Omega \rightarrow \Omega_i$ is defined as the $i^\text{th}$ coordinate mapping (e.g. $\pi_i(\omega_1, \omega_2,\ldots) = \omega_i$).
 \end{definition}


\begin{theorem}[{\bf Clump $f_i$ into a vector map}]
\label{thm: clump}
Let $(\Omega, \mathcal F)$ be a measurable space.
Let $(\Omega_i, \mathcal F_i)$ be a collection of measurable spaces indexed by an arbitrary index set $\mathcal I$ and let $f_i:\Omega\rightarrow \Omega_i$. Define $\vec f\colon \Omega \rightarrow \prod_{i\in \mathcal I} \Omega_i$ to be the map which sends $\omega \overset{\vec f}\mapsto (f_i(\omega))_{i\in\mathcal I}$. Then
\[
\text{$\vec f$ is $\mcirc \mathcal F / \textstyle\bigotimes_{i\in\mathcal I} \mathcal F_i$}
\Longleftrightarrow
\text{each $f_i \mcirc \mathcal F/\mathcal F_i$}.
\] Moreover, $\sigma\langle f_i,\mathcal F_i\colon i\in\mathcal I \rangle = \vec f^{-1}\bigl(\bigotimes_{i\in\mathcal I} \mathcal F_i\bigr)$.
\end{theorem}


\begin{theorem}[{\bf Measurable with respect to $\sigma\langle f_i \rangle$}]
\label{thm: f = g(vec f)}
Using the same assumptions and notation as in Definition \ref{def: sig generated by funs},
a function $f\colon\Omega\rightarrow \bar{\Bbb R}$ is measurable $\sigma\langle  f_i, \mathcal F_i:i\in \mathcal I \rangle$ if and only if there exists a function $g\colon \prod_{i\in \mathcal I} \Omega_i\rightarrow \bar{\Bbb R}$ which is measurable $\bigotimes_{i\in\mathcal I}\mathcal F_i$ and  $f= g((f_{i})_{i\in\mathcal I})$.
\end{theorem}
\begin{proof}
($\Longleftarrow$) This follows directly from Theorem \ref{thm: clump} (clump theorem) and Theorem \ref{thm: composition of measurable} (composition of measurable is measurable).

($\Longrightarrow$) This follow by an application of the $1-2-3$ argument. In particular, we show the result for simple function, then extend by taking point-wise limits. To start let $\vec f := (f_i)_{i\in \mathcal I}$ denote the clumped vector map. To summarize what is know from the assumptions and  \ref{thm: clump}
\begin{itemize}
\item $f\colon \Omega \rightarrow \bar{\Bbb R}$ is $\mcirc \sigma\langle f_i, \mathcal F_i\colon i\in\mathcal I \rangle/\mathcal B$;
\item $\vec f\colon \Omega \rightarrow \prod_{i\in\mathcal I}\Omega_i$ is $\mcirc \sigma\langle f_i, \mathcal F_i\colon i\in\mathcal I \rangle/\bigotimes_{i\in \mathcal I} \mathcal F_i$.
\item $\sigma\langle f_i, \mathcal F_i\colon i\in\mathcal I \rangle = \vec f^{-1}\bigl(\bigotimes_{i\in \mathcal I} \mathcal F_i\bigr)$
\end{itemize}


Suppose that $f=\sum_{k=1}^n c_k I_{A_k}$ for $A_k\in \sigma\langle f_i, \mathcal F_i\colon i\in\mathcal I \rangle = \vec f^{-1}\bigl(\bigotimes_{i\in \mathcal I} \mathcal F_i\bigr)$. Since $\sigma\langle f_i, \mathcal F_i\colon i\in\mathcal I \rangle = \vec f^{-1}\bigl(\bigotimes_{i\in \mathcal I} \mathcal F_i\bigr)$ we can write $A_k= \vec f^{-1} (F_k)$ where $F_k\in \bigotimes_{i\in \mathcal I} \mathcal F_i$. Now
\begin{align*}
f=\sum_{k=1}^n c_k I_{A_k} =\sum_{k=1}^n c_k I_{\vec f^{-1} (F_k)} =\sum_{k=1}^n c_k I_{F_k} \circ \vec f= g \circ \vec f
\end{align*}
where $g:=\sum_{k=1}^n c_k I_{F_k}$.
Certainly $g$ is $\mcirc \bigotimes_{i\in \mathcal I} \mathcal F_i / \mathcal B$ as was to be shown.

To finish let $f$ be a arbitrary $\mcirc \sigma\langle f_i, \mathcal F_i\colon i\in\mathcal I \rangle/\mathcal B$ function. By Theorem \ref{thm: structure thm} (the structure theorem) we can write $f(\omega) = \lim f_n(\omega)$ where $f_n$ are bounded simple functions. Therefore, from the preivous case, there exists $g_n$ which are $\mcirc \bigotimes_{i\in \mathcal I} \mathcal F_i / \mathcal B$ and $f_n(\omega) = g_n(\vec f(\omega))$. We definitely have that $f(\omega) = \lim f_n(\omega) = \lim g_n(\vec f(\omega))$ at each $\omega$. However we are not exactly done since there is no reason that $\lim_n g_n(v)$ exists for $v$'s which are not of the form $v=\vec f(\omega)$ (and therefore setting $g(v):=\lim g_n(v)$ only defines $g$ on the range of $\vec f$ ). To get around this set
\[
g(v):=\begin{cases}
\limsup_ng_n(v), &\text{when $\limsup_ng_n(v) =  \liminf_ng_n(v)$}\\
0, &\text{otherwise}.
\end{cases}
\]
This $g$ definitely satisfies $f = g \circ \vec f$ and it is measurable since $\limsup_ng_n(v)$ is measurable (since the $g_n$'s are and the closure properties of measurable functions) and the event $\{v\colon \limsup_ng_n(v) =  \liminf_ng_n(v)  \}$ is also measurable.
\end{proof}



The following is a corollary of Theorem \ref{thm: f = g(vec f)} which will be important when we define conditional expected value. In particular $E(X | Y_1,\ldots, Y_n)$ will be required to be measurable with respect to $\sigma\langle Y_1, \ldots, Y_n\rangle$. The following corollary says that $E(X | Y_1,\ldots, Y_n)$ must then be of the form $g(Y_1,\ldots, Y_n)$ where $g$ is Borel measurable.
\begin{corollary}
\label{cor: funs of measurable funs}
Let $X, Y_1, \ldots, Y_n$ be functions which map $\Omega$ into $\bar{\Bbb R}$.
Then
\[
\text{$X$ is  $\mcirc \sigma\langle  Y_1, \ldots, Y_n \rangle \Longleftrightarrow X = g(Y_1,\ldots, Y_n)$ where $g$ is $\mcirc$}.
\]
\end{corollary}



\begin{exercise}
Show that $\textstyle\bigotimes_{i\in \mathcal I} \mathcal F_i$ equals $\sigma\left\langle \Pi_{h\in \mathcal H} B_h \colon  B_h\in \mathcal F_h, \text{countable $\mathcal H\subset \mathcal I$}\right\rangle $.
\end{exercise}

\begin{exercise} Show that
$\mathcal B^{{\Bbb R}^d}= \bigotimes_{i=1}^d \mathcal B^{{\Bbb R}}$ and $\mathcal B^{\bar{\Bbb R}^d}= \bigotimes_{i=1}^d \mathcal B^{\bar{\Bbb R}}$
\end{exercise}




%%%%%%%%%%%%%%%%%
%
%  random variable subsection
%
%%%%%%%%%%%%%%%%%%%%%
\subsection{Application: random variable independence}




\begin{sectionassumption} For the rest of this section let $(\Omega, \mathcal F, P)$ denote a probability space.
\end{sectionassumption}





\begin{definition}[{\bf Independence for random variables}]
A collection of random variables $\{X_i: i\in\mathcal I\}$ are said to be {\bf independent} if and only if the collection of $\sigma$-fields $\{\sigma\langle X_i\rangle\colon i\in\mathcal I \}$ are independent.
\end{definition}




\begin{theorem}[{\bf ANOVA for random variables}]
Consider the following array of random variables all defined on the same probability space $(\Omega, \mathcal F, P)$
\[
\begin{array}{ccc}
 X_{1,1} &  X_{1,2} & \cdots \\
 X_{2,1} & X_{2,2} & \cdots \\
 X_{3,1} & X_{3,2} & \cdots \\
\vdots & \vdots & \ddots
\end{array}
\]
Each row may have a different number of columns (finite or infinite) and the number of rows may be finite or infinite.
Let $\mathscr R_1,\mathscr R_2,\ldots$ denote the $\sigma$-fields generated by the rows: $\mathscr R_i:=\sigma\langle X_{i,1}, X_{i,2} , \cdots \rangle $.
 Then the full collection  $\{ X_{i,k} \}$ of random variables are independent  if and only if the following two statements hold:
\begin{enumerate}
\item The random variables within each row are independent;
\item The  $\sigma$-fields generated by the rows, $\mathscr R_1,\mathscr R_2,\ldots$, are independent.
\end{enumerate}
\end{theorem}

% The following theorem gives the first nontrivial extension to the finite dimensional product measures in Definition \ref{def: Product measure of higher order}. For the existence of stochastic processes we need even more.

\begin{theorem}[{\bf Existence of independent $X_1, X_2,\ldots$}]
\label{thm: existance of independent rvs}
Let $\mu_1,\mu_2,\ldots$ be a finite or infinite sequence of probability measures on $(\Bbb R, \mathcal B^{\Bbb R})$. Then there exists on some probability space $(\Omega, \mathcal F, P)$ a sequence of independent random variables $X_1,X_2,\ldots$ such that $X_i$ has distribution $\mu_i$ for each $i$.
\end{theorem}







%\begin{definition}[{\bf Tail $\sigma$-field generated by r.v.s}]
%Let $X_1, X_2, \ldots$ be an infinite sequence of random variables on a probability space $(\Omega, \mathcal F, P)$. The tail $\sigma$-field generated by the $X_1, X_2, \ldots$ is defined as follows:
%\[\mathcal T:= \bigcap_{n=1}^\infty \sigma\langle X_n, X_{n+1},\ldots \rangle.  \]
%\end{definition}

\begin{theorem}[{\bf Kolmogorov's 0-1 law for random variables}]
Let $X_1, X_2, \ldots$ be an infinite sequence of {\sl independent} random variables on a probability space $(\Omega, \mathcal F, P)$. Then all tail events in the tail $\sigma$-field $\mathcal T:= \bigcap_{n=1}^\infty \sigma\langle X_n, X_{n+1},\ldots \rangle$ have probability either $0$ or $1$ and all functions $f:\Omega \rightarrow \bar{\Bbb R}$ which are $\mcirc \mathcal T/\mathcal B$ are almost surely constant.
\end{theorem}
%\subsection{Some concentration inequalities}

\begin{shaded}
\begin{definition}[{\bf Symmetric function}]
Let $X_1,X_2,\ldots$ be a sequence of independent identically distributed random variables defined on some probability space $(\Omega, \mathcal F, P)$.  Another random variable $Y$ on $(\Omega, \mathcal F, P)$ is said to be a {\bf symmetric function} of the $X_n$'s if $Y=f(X_1,X_2,\ldots)$ where $f\colon\Bbb R^\infty \rightarrow \Bbb R$ is $\mcirc \bigotimes_{i=1}^\infty \mathcal B^{\Bbb R}/\mathcal B^{\Bbb R}$ and $f(x_1,x_2,\ldots)=f(x_{\pi_1},x_{\pi_2},\ldots)$ whenever $\pi$ is a permutation that permutes a finite number coordinates. We say an event $A\in\mathcal F$ {\bf depends symmetrically} on the $X_n$'s if the indicator function $I_A(w)$, for $w\in\Omega$, is a symmetric function of the $X_n$'s.
\end{definition}
\end{shaded}

\begin{shaded}
\begin{theorem}[{\bf Hewitt-Savage 0-1 law}]
Let $X_1,X_2,\ldots$ be a sequence of independent identically distributed random variables defined on some probability space $(\Omega, \mathcal F, P)$. Each event that depends symmetrically on the $X_n$'s has probability 0 or 1, and each random variable that is a symmetric function of the $X_n$'s is almost surely constant
\end{theorem}
\end{shaded}



\begin{exercise}
Suppose that $Y_1, Y_2, \ldots$ is an infinite sequence of independent random variables, all defined on the same probability space $(\Omega, \mathcal F, P)$, taking the values $0$ and $1$ with probability $1/2$ each. Show that $U:= \sum_{k=1}^\infty 2^{-k} Y_k$ is uniformly distributed on $[0,1]$. Hint: show
\[
P[U \leq x] =\begin{cases}
x & \text{when $x\in [0,1]$}; \\
1 & \text{when $x>1$}; \\
0 & \text{when $x<0$}.
\end{cases}
\] for all $x\in \Bbb R$ by analyzing $P[U_n \leq x]$ as $n\rightarrow \infty$ where $U_n:=\sum_{k=1}^n 2^{-k} Y_k$.
\end{exercise}

\begin{exerciseproof}

We first notice that $U:= \sum_{k=1}^\infty 2^{-k} Y_k$  is a well defined random variable because:
\begin{itemize}
\item $\sum_{k=1}^\infty 2^{-k} Y_k(\omega)$ is defined for all $\omega\in\Omega$ since the sumands are positive.
\item  $\sum_{k=1}^\infty 2^{-k} Y_k$  is measurable since
\begin{align*}
\text{$Y_k$'s are  $\mcirc$ }
&\Longrightarrow \text{$\sum_{k=1}^n 2^{-k} Y_k$ is $\mcirc$ by closure thm}\\
&\Longrightarrow \text{$\limsup_{n\rightarrow \infty}\sum_{k=1}^n 2^{-k} Y_k$ is  $\mcirc$ by closure thm} \\
&\Longrightarrow \text{$\sum_{k=1}^\infty 2^{-k} Y_k$ is  $\mcirc$.}
\end{align*}
\item $\sum_{k=1}^\infty 2^{-k} Y_k(\omega)\leq \sum_{k=1}^\infty 2^{-k} = 1$. In particular $U$ takes values in $\Bbb R$.
\end{itemize}
This gives that $U$ is a well defined random variable.

Next lets analyze the random variable $U_n:=\sum_{k=1}^n 2^{-k} Y_k$. Notice first that $U_n$ takes values in the dyadic integers of rank $n$, $\{0,\ldots, \frac{k}{2^n},\ldots, \frac{2^n-1}{2^n}\}$. Each dyadic integer corresponds to a unique tuple of values for $(y_1,\ldots, y_n)\in \{0,1\}^n$. Notice that also that for any $(y_1,\ldots, y_n)\in \{0,1\}^n$ we have
\[P[Y_1 = y_1, \ldots, Y_n=y_n] = \frac{1}{2^n}  \]
by independence and the marginal distributions of $Y_k$. Therefore $U_n$ is uniform on $\mathcal Y_n:= \{0,\ldots, \frac{k}{2^n},\ldots, \frac{2^n-1}{2^n}\}$. Indeed when $x\in [0,1]$ we have
\begin{align*}
P[U_n \leq x ]
&= \sum_{u\in \mathcal Y_n: u\leq x}  \frac{1}{2^n} \\
&= \frac{\# \{u\in \mathcal Y_n: u\leq x\}}{2^n} \\
&= \frac{x2^n +O(1)}{2^n} \\
&\rightarrow x,\quad\text{as $n\rightarrow \infty$}
\end{align*}

To finish  suppose $x\in [0,1]$. Notice that $U_n\uparrow U$ which implies $\{U_n\leq x\}\downarrow \{U\leq x \}$ (remark: it is {\em not} true that $\{U_n< x\}\downarrow \{U< x \}$). Therefore by CFA for probability measures
\begin{align*}
P[U\leq x] =  \lim_n P[U_n\leq x] = x
\end{align*}
Since $U$ only takes values in $[0,1]$ we must have $P[U\leq x] = 0$ when $x<0$ and $P[U\leq x] = 1$ when $x>1$.

\end{exerciseproof}


Suppose $X$ and $Y$ are two random variables, not necessarily defined on the same probability space. $Y$ is said to be {\bf stochastically larger} than $X$ if $P[X\leq x]\geq P[Y\leq x]$ for all $x\in \Bbb R$.

\begin{exercise}
Suppose $X$ and $Y$ are random variables and that $Y$ is stochastically larger than $X$. Show there exists random variables $X^*$ and $Y^*$ defined on a common probability space $(\Omega, \mathcal F, P)$ such that $X^*\sim X$, $Y^*\sim Y$ and $X^*(\omega)\leq Y^*(\omega)$ for all $\omega\in \Omega$.
\end{exercise}
\begin{exerciseproof}
Let
\begin{align*}
F_X(x)&:=P[X\leq x] \\
F_Y(x)&:=P[Y\leq x]
\end{align*}
be the distribution functions for $X$ and $Y$, respectively. Let $(\Omega^*, \mathcal F^*, P^*) = ((0,1), \mathcal B^{(0,1)}, \mathcal L^1)$ be the uniform probability measure on $(0,1)$. Let $X^*(\omega) :=F_X^{-1}(\omega)$ and $Y^*(\omega):= F_Y^{-1}(\omega)$ be random variables on $\Omega^*$ such that $X^*\sim X$ and $Y^*\sim Y$.

By assumption, $F_Y(x)\leq F_X(x)$ for all $x\in\Bbb R$. All we need to show is $F_X^{-1}(\omega)\leq F^{-1}_Y(\omega)$ for all $\omega\in \Omega^*$. Since
\begin{align*}
F_X^{-1}(\omega)&:=\inf\{x: F_X(x)\geq \omega \} \\
F_Y^{-1}(\omega)&:=\inf\{x: F_Y(x)\geq \omega \}
\end{align*}
all we need to show is that $\{x: F_Y(x)\geq \omega \} \subset \{x: F_X(x)\geq \omega \}$ which clearly follows by $F_Y(x)\leq F_X(x)$.
\end{exerciseproof}



Let $\mathcal I$ be an arbitrary index set and let $X_i, i\in \mathcal I$ be a family of random variables where each $X_i$ is defined on a probability space $(\Omega_i, \mathcal F_i, P_i)$. Let $F_i(x):= P_i(X_i\leq x)$ be the distribution function of $X_i$. The $X_i$'s  are said to be {\bf stochastically dominated} by a random variable $X$ if $X$ is stochastically larger than $|X_i|$ for each $i\in \mathcal I$. The $X_i$'s are said to be {\bf pointwise dominated} by  $X$ if all the random variables $X, X_i$, for $i\in \mathcal I$, are defined on the same probability space and $|X_i(\omega)|\leq X(\omega)$ for each $w\in \Omega$ and for each $i\in \mathcal I$.
The distribution functions $F_i$ are said to be {\bf tight} if the following two equalities hold
\begin{align*}
\lim_{x\rightarrow-\infty} \sup_{i\in \mathcal I} F_i(x) &= 0 \\
\lim_{y\rightarrow+\infty} \inf_{i\in \mathcal I} F_i(y) &= 1.
\end{align*}

\begin{exercise}
Let
$X_i, i\in \mathcal I$, be a family of random variables where each $X_i$ is defined on a probability space $(\Omega_i, \mathcal F_i, P_i)$. Show that the following are equivalent
\begin{enumerate}
\item\label{ex: stcho: item 1} The $X_i$'s  are stochastically dominated by some random variable;
\item\label{ex: stcho: item 2} The corresponding distribution functions $F_i$ are tight;
\item\label{ex: stcho: item 3} There exists random variables $X^*_i$, $i\in \mathcal I$, all defined on a common probability space such that $X^*_i\sim X_i$ for each $i\in \mathcal I$ and the $X_i^*$'s are pointwise dominated by some random variable.
\end{enumerate}
\end{exercise}
\begin{exerciseproof}
(\ref{ex: stcho: item 2} $\Longrightarrow$ \ref{ex: stcho: item 3}) Suppose $F_i$'s are tight.  Let the probability space $(\Omega^*, \mathcal F^*, P^*) = ((0,1), \mathcal B^{(0,1)}, \mathcal L^1)$ be the uniform probability measure on $(0,1)$. For each $\omega\in\Omega^*$ define
\begin{align*}
X_i^* &:= F_i^{-1}(\omega)\\
X^*   &:= \sup_{i\in \mathcal I} (X_i^*)^+ + \sup_{i\in \mathcal I} (X_i^*)^-.
\end{align*}
{\em Remark:} the reason we are defining $X^*$ this way is that it is not clear how to show $\sup_{i\in \mathcal I} |X_i^*|$ is measurable. We need to show
\begin{itemize}
\item[$(i)$] $|X^*_i(\omega)|\leq X^*(\omega)$ for all $\omega\in \Omega^*$.
\item[$(ii)$] $X^*$ takes values in $\Bbb R$.
\item[$(iii)$] $X^*$ is measurable.
\end{itemize}
Clearly $(i)$ holds.
Let's see why $(iii)$ is true. For each $i\in\mathcal I$, $F^{-1}_i(\omega)$ is non-decreasing in $\omega \in (0,1)$ and hence $(F^{-1}_i(\omega))^+$ is non-decreasing and   $(F^{-1}_i(\omega))^-$  is non-increasing. Therefore $\sup_{i\in \mathcal I} (F_i^{-1}(\omega))^+$ is non-decreasing and $\sup_{i\in \mathcal I} (F_i^{-1}(\omega))^-$ is non-increasing and therefore measurable.\\
To see why $(ii)$ is true suppose there exists an $\omega\in\Omega^*$ such that either
\[
 \sup_{i\in \mathcal I} (F_i^{-1}(\omega))^+=\infty \qquad\text{or}\qquad \sup_{i\in \mathcal I} (F_i^{-1}(\omega))^- = \infty.
\]
Now
\begin{align*}
\sup_{i\in \mathcal I} (F_i^{-1}(\omega))^+=\infty
&\Longrightarrow \text{$\exists i_n\in\mathcal I$ s.t. $F_{i_n}^{-1}(\omega)^+>n, \forall n$ }\\
&\Longrightarrow \text{$\exists i_n\in\mathcal I$ s.t.  $F_{i_n}^{-1}(\omega)>n, \forall n$} \\
&\Longrightarrow \text{$\exists i_n\in\mathcal I$ s.t.  $\omega>F_{i_n}(n), \forall n$}\\
&\qquad\qquad\qquad\qquad\text{the switching formula.}
\end{align*}
This contradicts the tightness assumption.

(\ref{ex: stcho: item 3} $\Longrightarrow$ \ref{ex: stcho: item 1}) Trivial.


(\ref{ex: stcho: item 1} $\Longrightarrow$ \ref{ex: stcho: item 2})
We are assuming $P(X\leq x)\leq P(|X_i|\leq x)$. To show  $\lim_{x\rightarrow-\infty} \sup_{i\in \mathcal I} F_i(x) = 0$ notice that when $x < 0$ we have
\begin{align*}
P(X_i\leq x) &= \text{left tail} \\
&\leq 1 - P(|X_i|\leq |x|) \\
&\rightarrow 0,\quad\text{as $x\rightarrow -\infty$.}
\end{align*}
The other direction follows noticing that when $y>0$
\begin{align*}
P(X_i\leq y) &= 1-\text{right tail} \\
&\geq P(|X_i|\leq y) \\
&\rightarrow 1,\quad\text{as $y\rightarrow \infty$.}
\end{align*}


\end{exerciseproof}
