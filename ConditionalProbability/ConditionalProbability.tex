


This part of the notes develops conditional expectation and probability. To motivate all this deep mathematics notice the following paradox. Suppose $X$ and $Y$ are independent random variables, each with a uniform distribution on $(0,1)$. For a Borel set $B$ consider how to compute $P[X\in B| X=Y]$. There are a number of ways one might approach this problem. First, define $Z := X-Y$,  use transformation of densities to find the density $f_{X, Z}(x,z)$, and then set $P[X\in B| X=Y]=P[X\in B| Z=0]=\frac{f_{X,Z}(x,0)}{f_Z(0)}$ where $f_Z$ is the marginal density of $Z$. Another way is to define $W := X/Y$ and then set $P[X\in B| X=Y]=P[X\in B| W=1]=\frac{f_{X,W}(x,1)}{f_W(1)}$. The big problem is that these give different answers. Even more astounding is that, in some sense, neither approach is fundamentally wrong.
\textcolor{red}{Add a detailed example where one observes a random field with either multiplicative numerical truncation and one with additive numerical truncation}


Here is a broad overview of what we are doing. Our basic strategy will be to define conditional expected value, $E^{\mathcal B}X$, with respect to a sub $\sigma$-field $\mathcal B \subset \mathcal F$. Once we have conditional expected value then we can get conditional probability by the expected value of indicators. The main tool for $E^{\mathcal B} X$ is the Radon-Nikodym derivative. In particular define suppose $X\in \mathscr N$ and let $\nu$ be defined by
\[ \nu[B] = \int_B X dP \]
for all $B\in \mathcal B$. Now we use the Radon-Nikodym theorem to get the existance of $d\nu/ dP$, which is $\mathcal B$ measurable, such that
\[ \int_B X dP = \int_B \frac{d\nu}{dP} dP. \]
$d\nu/ dP$ then serves as $E^{\mathcal B}X$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section
%
%%%%%%%%%%%%%%%%%%%%%%
\section{Radon-Nikodym derivatives}





\begin{definition}[{\bf Absolute continuity and singularity}]
Let $\nu$ and $\mu$ be measures on $(\Omega, \mathcal A)$.
\begin{itemize}
\item
$\nu$ and $\mu$ are said to be {\bf singular}, denoted $\nu \perp \mu$, if and only if there exists a set $A\in\mathcal A$ such that
\[
\nu(A^c) =0 = \mu(A).
\]
\item
$\nu$ is said to be {\bf absolutely continuous with respect to $\mu$}, denoted $\nu \ll \mu$, if and only if
\[\text{$\nu(A)=0$ for every $\mathcal A$-set $A$ for which $\mu(A)=0$.}  \]
\end{itemize}
\end{definition}

The following theorem and proof is probably one of my favorite in all of probability/measure theory.
\begin{theorem}[{\bf Radon-Nikodym}]
\label{thm: rn1}
Let $\mu$ and $\nu$ be two $\sigma$-finite measures on $(\Omega, \mathcal A)$. If $\nu \ll \mu$ then there exists a measurable function  $\frac{d\nu}{d\mu} \in \mathscr N$  such that
\[
\nu[A] = \int_A \frac{d\nu}{d\mu}\, d\mu
\]
for all $A\in \mathcal A$. Moreover, $\frac{d\nu}{d\mu}$ is $\mu$-unique.
\end{theorem}

\begin{proof}
Notice first that $\mu$-uniqueness follows directly from the uniqueness of densities Theorem \ref{thm: uniqueness of densities}.

The existent of $\frac{d\nu}{d\mu}$ is trivially true if either $\mu$ or $\nu$ is identically zero. So, from now on, suppose both are not identically zero.
We are in search of $\frac{d\nu}{d\mu}$. The non-trivial assumption and the $\sigma$-finite assumption on both $\mu$ and $\nu$  allows us to apply our world view Theorem  \ref{thm: world view} and establish the existence of two probability measures $P$ and $Q$ on $(\Omega, \mathcal A)$ such that $\frac{d\mu}{dP}$ and $\frac{d\nu}{dQ}$ both exist and map into $(0,\infty)$. Moreover, Exercise \ref{ex: 1/density} says $\frac{dP}{d\mu} = 1/\frac{d\mu}{dP}$ and  $\frac{dQ}{d\nu} = 1/\frac{d\nu}{dQ}$. Then notice that the chain rule Theorem
\ref{thm: chain rule for sigma finite} says that if $\frac{dQ}{dP}$ exists then so does $\frac{d\nu}{d\mu}$ and can be computed as follows
\[ \frac{d\nu}{d\mu}= \frac{d\nu}{dQ} \,\frac{dQ}{dP}\, \frac{dP}{d\mu}.\]
Therefore we have reduced the problem to finding $\frac{dQ}{dP}$ where $Q$ and $P$ are probability measures such that $Q \ll P$ (this last condition follows since the existence of $\frac{d\mu}{dP}$ and $\frac{dQ}{d\nu}$ implies $Q\ll \nu \ll \mu \ll P$).
Consider the probability measure $W = (Q + P)/2$.
The construction of $W$ ensures $Q \ll W$ and $P \ll W$. The idea is that we'll use Reisz to get $\frac{dP}{dW}$ and $\frac{dQ}{dW}$ then show that $\frac{dQ}{dP} = \frac{dQ}{dW} / \frac{dP}{dW}$.


Define the following functionals over over $L^2(W):=\{X\colon \int X^2 dW <\infty \}$
\begin{align*}
f_P(X) &:= \int X dP \quad\text{and}\quad f_Q(X) := \int X dQ.
\end{align*}
By Exercise \ref{ex: for rn1 thm}, both $f_P$ and $f_Q$ are continuous linear functionals over $L^2(W)$. By Riesz's Theorem \ref{thm: riesz} there exists random variables $p$ and $q$ such that
\begin{align*}
f_P(X) &= \langle p,  X\rangle = \int p X dW\\
f_Q(X) &= \langle q,  X\rangle = \int q X dW
\end{align*}
for all $X\in L^2(W)$. In the case when $X= I_A$ we have
\begin{align*}
P[A] &= f_P(I_A) = \int_A p  \,dW \\
Q[A] &= f_Q(I_A) = \int_A q  \,dW.
\end{align*}
Therefore $p = \frac{dP}{dW}$ and $q = \frac{dQ}{dW}.$
In what follows I will analyze the ratio $q/p$ but I want to avoid $\infty /\infty$. I can ensure this is avoided by noticing that since $P,Q$ and $W$ are all probability measures, $p$ and $q$ must takes values in $[0,\infty)$ with $W$-probability one. Therefore I may, and do, modify $p$ and $q$ over $W$-null sets so that they never take on the value $\infty$.

To finish we show that modifying $q/p$ on the appropriate  set serves as $\frac{dQ}{dP}$. Define
$N:= \{p=0\}$ and set
\begin{equation}
\frac{dQ}{dP}:=\begin{cases}
q/p & \text{on $N^c$} \\
0  & \text{on $N$}.
\end{cases}
\end{equation}
Now $\frac{dQ}{dP}$ has the right properties since
%In particular since $p = \frac{dP}{dW}$ takes values in $(0,\infty)$ $W$-a.e. we have that $1/p = \frac{dW}{dP}$ (by Exercise ...).
\begin{align*}
\int_A \frac{dQ}{dP} dP &= \int_{A\cap N^c} (q /p) \,dP \\
&= \int_{A\cap N^c} (q/p) p\,dW,\,\,\text{by Theorem \ref{thm: slap}}  \\
&= \int_{A\cap N^c} q\, dW,\,\,\text{since $(q/p) p=q$ on $N^c$.}   \\
& = Q[A\cap N^c] + Q[A\cap N],\\
&\qquad\qquad \text{since $Q[A\cap N]=0$ by $P[N]=0$ and $Q\ll P$ }\\
& = Q[A].
\end{align*}
\end{proof}


\begin{definition}
For two measures $\nu, \mu$ on $(\Omega, \mathcal A)$ define the notation $\nu \lll \mu$ to mean that $\nu\ll \mu$ and $\mu$ is $\sigma$-finite.
\end{definition}

\begin{theorem}[{\bf Radon-Nikodym$^*$}]
\label{thm: rn2}
Theorem \ref{thm: rn1} is still true under the weaker assumption $\nu \lll \mu$.
\end{theorem}
\begin{proof}
The problem with this case is that we are no longer guaranteed that $Q$ exists (in the proof of Theorem \ref{thm: rn1}). But we still have the existence of $P$ and $\frac{dP}{d\mu}$. Moreover for this $P$ we have $\nu \ll \mu \ll P$. Therefore all we need is to show that there exists $\frac{d\nu}{dP}$ under the assumption $\nu \ll P$ and we can then construct $\frac{d\nu}{d\mu}$ by
\[\frac{d\nu}{d\mu} = \frac{d\nu}{dP}\,\frac{dP}{d\mu}.  \]


We will look for a set $F\in \mathcal A$ which has the property that $\nu_F[\cdot]:=\nu[\cdot \cap F]$ is $\sigma$-finite and $\nu[A \cap F^c] = \infty P[A\cap F^c]$. Once we have such a set we can use Theorem \ref{thm: rn1} to construct $\frac{d\nu_F}{dP}$ and define  $\frac{d\nu}{dP}:=\frac{d\nu_F}{dP} + \infty I_{F^c}$. This $\frac{d\nu}{dP}$ has the requred properties since
\begin{align*}
\int_A\Bigl[ \frac{d\nu_F}{dP} + \infty I_{F^c} \Bigr] dP  &= \int_A \frac{d\nu_F}{dP}dP + \int_A\infty I_{F^c} dP \\
& = \nu_F[A] + \infty P[A\cap F^c] \\
& = \nu[A\cap F] + \nu[A \cap F^c] \\
& = \nu[A].
\end{align*}

To construct such an $F$ we  find the biggest ``$\sigma$-finite set'' as follows
\begin{align*}
\mathcal F &:= \{\cup_{i=1}^\infty A_i\colon \text{$A_i\in \mathcal A$ and $\nu[A_i]<\infty$ for all $i$ } \}\\
m &:= \sup\{P[F]: F\in \mathcal F  \}.
\end{align*}
The $F$ we want to construct is the one that attains the above supermum.


To find it let $F_n\in \mathcal F$ such that $P[F_n]\rightarrow m$ and define $F:= \cup_{n=1}^\infty F_n$.  Now since $\mathcal F$ is clearly closed under countable union (since the countable union of a countable unions is again a countable union) we have $F\in \mathcal F$ so that
\begin{align*}
m \leftarrow P[F_n]\leq P[F] \leq m
\end{align*}
which implies $m = P[F]$.
%Since $F\in \mathcal F$ there exists $A_i\in\mathcal A$ such that $\nu[A_i]<\infty$ and $F = \cup_{i=1}^n A_i$.

Now lets see that $F$ has the desired properties. We can immediately see that $\nu[\cdot \cap F]$ is $\sigma$-finite using the cover $F^c, A_1, A_2,\ldots$ where the $A_i$'s come from the fact that $F\in\mathcal F$ so that  $F=\cup_{i=1}^\infty A_i$ for $\nu[A_i]<\infty$. To finish we just need to show, $\nu[A \cap F^c] = \infty P[A\cap F^c]$, which is equivalent to the following  equalities
\begin{align}
\label{eq: rna} P[A\cap F^c] = 0 &\Longrightarrow \nu[A\cap F^c]=0 \\
\label{eq: rnb} P[A\cap F^c] > 0 &\Longrightarrow \nu[A\cap F^c]=\infty.
\end{align}
Equation (\ref{eq: rna}) follows directly from the fact that $\nu \ll P$. We show  (\ref{eq: rnb}) by contradiction. Assume $P[A\cap F^c] > 0$ but also $\nu[A\cap F^c]<\infty$. Therefore $(A\cap F^c) \cup F \in \mathcal F$. But this contradict the maximal property of $P[F]$ as follows
\[m = P[F] < P[A\cap F^c] +  P[F]  = P[(A\cap F^c) \cup F] \leq m.  \]
Note, the above inequality is where we use the fact that $P$ is a probability measure.
This is a contradiction and therefore (\ref{eq: rnb}) holds.
\end{proof}


\begin{theorem}[{\bf Properties of Radon-Nikodym derivatives}]
\label{thm: props of rn}
Suppose $\mu, \sigma, \nu, \nu_1, \nu_2, \ldots$ are measures on $(\Omega, \mathcal A)$.


\begin{enumerate}
\item Suppose $\nu_1,\nu_2 \lll\mu$ for $c_1, c_2 \geq 0$. Then $c_1\nu_1 +c_2\nu_2 \ll \mu $ and
\[
\frac{d(c_1\nu_1 +c_2\nu_2 )}{d\mu} =  c_1\frac{d\nu_1 }{d\mu} +  c_2\frac{d\nu_2}{d\mu}\quad \text{$\mu$-a.e.}
\]
\item
Assume $\nu_1,\nu_2 \lll \mu$. Then
\[\text{$\nu_1 \leq \nu_2$ if and only if $\displaystyle\frac{d\nu_1}{d\mu} \leq \frac{d\nu_2}{d\mu}$ $\mu$-a.e.}\]
\item If $\nu_n[A]$ is non-decreasing for each $A\in \mathcal A$ and  $\nu_n \lll \mu$ then
\[ \frac{d\nu_n}{d\mu} \overset{\text{$\mu$-a.e.}}\longrightarrow\frac{d\nu}{d\mu} \]
where $\nu[A]:=\lim_n \nu_n[A]$.
\item
\label{item: finite rn}
 Assume $\nu \lll \mu$. Then
\[ \text{$\nu$ is finite if and only if $\frac{d\nu}{d\mu}$ is $\mu$-integrable}\]
\item Assume $\nu \lll \mu$. Then
\[\text{$\nu$ is $\sigma$-finite if and only if $\frac{d\nu}{d\mu}<\infty$ \, $\mu$-a.e.}\]
\item If  $\nu \lll \sigma$, $\sigma \lll \mu$  and $\mu$ is $\sigma$-finite then $\nu \ll \mu$ and
\[\frac{d\nu}{d\mu} =  \frac{d\nu}{d\sigma}\frac{d\sigma}{d\mu} \]
$\mu$-a.e.
\item If $\mu, \nu \lll \sigma$  then
\[
\frac{d\nu}{d\mu} =\begin{cases}
\frac{d\nu}{d\sigma} / \frac{d\mu}{d\sigma} & \text{on $\{\omega\colon\frac{d\mu}{d\sigma}(\omega) > 0 \}$} \\
0 & \text{otherwise}
\end{cases}
 \]
$\mu$-a.e.
\item If  $\mu\lll \nu$  and $\nu\lll \mu$ then $\frac{d\nu}{d\mu} > 0$ $\mu$-a.e. and
\[\frac{d\mu}{d\nu} = \frac{1}{d\nu/d\mu}  \]
$\nu$-a.e.
\end{enumerate}
\end{theorem}
\begin{proof} See Exercise \ref{ex: for rn2 thm}.
\end{proof}





\begin{theorem}[{\bf Lebesgue decomposition}]
Let $P$ and $Q$ be two probability measures on $(\Omega, \mathcal A)$. There exists a $P$-null set $N$ and a function $\delta\in \mathscr N$  such that
\[
Q[A] = \int_A\delta dP +  Q[A\cap N] =: Q_a[A] + Q_s[A]
\]
for all $A\in \mathcal A$.
$N$ is $Q$-unique, $\delta$ is $P$-unique and $Q = Q_a + Q_s$ is the unique decomposition of a absolutly continuous measure with respect to $P$  and a singluar measure with respect to $P$. Moreover, $\delta$ is the $P$-largest $\delta\in \mathscr N$ such that $\int_A \delta dP \leq Q[A]$ for all $A\in \mathcal A$.
\end{theorem}



\begin{shaded}
\begin{definition}[{\bf Signed measure}]
If $\mathcal A$ is a $\sigma$-field of $\Omega$-sets, then $\mu:\mathcal \mathcal A\rightarrow \overline{\Bbb R}$ is a {\bf signed measure} if $\mu(\varnothing)=0$ and  $\mu\bigl( \bigcup_{k=1}^\infty A_k \bigr)=\sum_{k=1}^\infty \mu(A_k)$ for all disjoint $A_1, A_2,\ldots \in\mathcal A$.
\end{definition}



\begin{theorem}[{\bf Hahn-Jordan decomposition}]
If $\mu$ is a signed measure on $(\Omega, \mathcal A)$ then one can write $\Omega$ as the disjoint unions of sets $S^+$ and $S^-$ such that
 \begin{itemize}
 \item $\mu(A)\geq 0$ for all $\mathcal A$-sets $A\subset S^+$
 \item $\mu(A)\leq 0$ for all $\mathcal A$-sets $A\subset S^-$.
 \end{itemize}
 Moreover $\mu$ has the following  decomposition
 \[
\mu[A] = \mu^+[A] - \mu^-[A]
 \]
where $\mu^\pm[B]= \sup\{\pm\mu[A]\colon A\subset B, A\in\mathcal A \}$.
\end{theorem}

\begin{proof}

Since $\mu$ can not take on both the values $-\infty$ and $\infty$ we can suppose wlog that $\mu$ does not assume the value $-\infty$.

%A set $A\in \mathcal A$ is said to be {\bf strictly positive} if $\mu[B]\geq 0$ for all $\mathcal A$-sets $B\subset A$. Similarly  $A\in \mathcal A$ is said to be {\bf strictly negative} if $\mu[B]\leq 0$ for all $\mathcal A$-sets $B\subset A$.

({\it Closure properties of strictly negative sets}) A set $S\in \mathcal A$ is said to be {\bf strictly negative} if $\mu[A]\leq 0$ for all $\mathcal A$-sets $A\subset S$. Notice first the somewhat trivial fact  that every subset of a strictly negative set is also strictly negative.
We also show that the set of all strictly negative sets is closed under countable union. To see why let $S_1, S_2 \ldots $ denote strictly negative sets. Set $S:= \cup_{i=1}^\infty S_i = \cup_{i=1}^\infty S_i^*$ where $S_i^*:= S_i - (S_1\cup\cdots\cup S_{i-1})$ and show that $S$ is strictly negative. Since $S_i$ is strictly negative so is $S^*_i$ (since it is a subset of $S_i$). Let $A\subset S$ be $\mathcal A$-measurable. Since the $S_i^*$ are disjoint we have that
\[
\mu[A]= \mu[A\cap S] = \textstyle\sum_{i=1}^\infty \underbrace{\mu[A\cap S_i^*]}_{\leq 0} \leq 0.
\]
Therefore $S$ is strictly negative as was to be shown.

({\it Construct $S^-$}) We will define $S^-$ to be a set which attains the following infimum
\[m:=\inf\{\mu[S]\colon \text{$S$ is a strictly negative $\mathcal A$-set} \}.  \]
 To see that such a set exists let $S_n$ be strictly negative sets such that $\mu[S_n]\rightarrow m$. Put $S^-:=\cup_{n=1}^\infty S_n$. Then $S^-$ and hence $S^- - S_n$ are both strictly negative by the closure properties of such sets. Therefore
\begin{align*}
m \leq \mu[S^-]
&= \mu[S_n\cup (S^- - S_n)]\\
&= \mu[S_n] + \mu[S^- - S_n] \\
&\leq \mu[S_n]\rightarrow m.
\end{align*}
Therefore $\mu[S^-]=m$ and hence the infimum is attained.


({\it Extraction lemma}) The extraction lemma says
\begin{quote}
{\it
For any $\mathcal A$-set $A$ there exists a strictly negative $\mathcal A$-set $N\subset A$ such that $\mu[N]\leq \mu[A]$.}
\end{quote}
This lemma is trivial for sets $A$ with positive measure (by choosing $N=\varnothing$). So suppose $\mu[A]< 0$. The idea is to repeatedly extract as much positive measure as possible from $A$, the remainder should strictly negative.
Set
\begin{equation}
\label{eq: pos part in proof}
\mu^+[A]:=\sup\{\mu[B]\colon B\subset A, B\in\mathcal A \}
\end{equation}
and recursively choose $\mathcal A$-sets $B_n$ such that
\begin{align}
&\underbrace{B_n \subset A - (B_1\cup \cdots \cup B_{n-1})}_\text{remove some of what is left} \\
&\underbrace{\mu[B_n] \geq \textstyle\frac{1}{2}\mu^+ [A - (B_1\cup \cdots \cup B_{n-1})]\wedge 1}_\text{make sure it has some positive measure}. \label{eq: wedge}
\end{align}
Note: the `$\wedge$' is there just to avoid having to deal with measure $=\infty$. Also the $1/2$ is in front of $\mu^+$ since we don't know that the $sup$ in (\ref{eq: pos part in proof}) is attained.
Now removing $\cup_{n=1}^\infty B_n$  from $A$ should give us a strictly negative set.
Define our candidate for the strictly negative set $N :=A - \cup_{n=1}^\infty B_n$. We just need to show $\mu^+[N]\leq 0$


 Notice that $\mu[A]=\mu[\cup_{n=1}^\infty B_n]+\mu[N]$. Remember that  $-\infty<\mu[A]<0$ by the assumption that $\mu[A]<0$ and that $\mu$ doesn't take the value $-\infty$. Therefore we have that $\mu[\cup_{n=1}^\infty B_n] =  \sum_{n=1}^\infty \mu[B_n]<\infty$ (we are using the fact that we picked the $B_n$'s to be disjoint). Therefore $\mu[B_n]\rightarrow 0$. This is key. First it says that for all large $n$ we can ignore the `$\wedge$' in (\ref{eq: wedge}) and have that $\mu[B_n] \geq \frac{1}{2}\mu^+ [A - (B_1\cup \cdots \cup B_{n-1})]$.  Second we use $\mu[B_n]\rightarrow 0$ to bound $\mu^+[N]$ as follows
 \begin{align*}
 \mu^+[N]
 &= \mu^+[A - \cup_{n=1}^\infty B_n] \\
 &\leq \mu^+[A - \cup_{n=1}^m B_n],\,\,\text{larger sup set} \\
 &\leq 2\mu[B_m],\,\,\text{discussed above}\\
 &\rightarrow 0.
 \end{align*}
Therefore $\mu^+[N]\leq 0$ and hence $N$ is strictly negative.


({\it Show  $S^+$ has the right properties}) Define $S^+:=  (S^-)^c$ and lets show it is strictly positive. Let $A\subset S^+$ and $A\in \mathcal A$. We need to  show $\mu[A]\geq 0$. Extract a strictly negative $N\subset A\subset S^+$ such that $\mu[N]\leq \mu[A]$. Notice that $N\cap S^- =\varnothing$ and $N\cup S^-$ is strictly negative. Therefore
\[
m\leq \mu[N\cup S^-]=  \mu[N]+ \mu[S^-]\leq \mu[A] + m
\]
which shows that $\mu[A]$ must be positive as was to be shown (note $\mu[N]\leq \mu[A]$ comes from extraction). Therefore $\mu[A]\geq 0$ as was to be shown.


% ({\it Uniqueness}) Let $\widetilde S^+$ and $\widetilde S^-$ denote a different decomposition with the desired properties. Then
% \begin{align}
% \widetilde S^- \Delta S^-
% &= [(\widetilde S^-)^c \cap S^-] \cup [\widetilde S^- \cap (S^-)^c] \\
% &= [\widetilde S^+ \cap S^-] \cup [\widetilde S^- \cap S^+]
% \end{align}
% where every subset of $[\widetilde S^+ \cap S^-]$ must be both strictly negative an strictly positive, and similary for $[\widetilde S^- \cap S^+] $. This implies $\mu[\widetilde S^- \Delta S^-]=0$. Similarly for $\mu[\widetilde S^+ \Delta S^+]=0$.

({\it Jordan decomposition}) Notice that
\begin{align*}
\mu[A]
% & = \mu[A\cap (S^+\cup S^-)] \\
& = \mu[A\cap S^+] + \mu[A\cap S^-] =:\mu^+[A] + \mu^-[A].
\end{align*}
We need to show
\begin{align*}
\phantom{-}\mu[A\cap S^+] & = \sup\{\phantom{-}\mu[B]\colon B\subset A, B\in \mathcal A  \} \\
-\mu[A\cap S^-] & = \sup\{-\mu[B]\colon B\subset A, B\in \mathcal A  \}.
\end{align*}
Since $B\subset A$ it will be sufficient to conclude that
\begin{align*}
\mu[B]
&= \mu[B\cap S^+] + \mu[B\cap S^-]\\
&\leq \mu[A\cap S^+] + \text{negative}
\end{align*}
and
\begin{align*}
-\mu[B] &= -\mu[B\cap S^+] - \mu[B\cap S^-]\\
&\leq \phantom{-}\text{\,\,negative\,\,} -  \mu[A\cap S^-].
\end{align*}
where the only thing we need is that $\mu[B\cap S^+]\leq \mu[A\cap S^+]$ and $\mu[B\cap S^-]\geq \mu[A\cap S^-]$. But these two inequalities are easy since $\mu[A\cap S^+] = \mu[(A-B)\cap S^+]+\mu[B\cap S^+]\geq \mu[B\cap S^+]$ and $\mu[A\cap S^-] = \mu[(A-B)\cap S^-]+\mu[B\cap S^-]\leq \mu[B\cap S^-]$.
\end{proof}



\end{shaded}





\begin{exercise}
\label{ex: for rn1 thm}
Referring to the proof of Theorem \ref{thm: rn1} show that $f_P$ and $f_Q$ are both continuous linear functionals over $L^2(W)$.
\end{exercise}
% use the 0-1-2 argument to show that $E_W X \geq 2 E_PX$ over all $X\in \mathcal N$.


\begin{exercise}
\label{ex: for rn2 thm}
Prove Theorem \ref{thm: props of rn}.
\end{exercise}
% use the 0-1-2 argument to show that $E_W X \geq 2 E_PX$ over all $X\in \mathcal N$.



\subsection{Application: $dP/dQ$ for random fields}




\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section
%
%%%%%%%%%%%%%%%%%%%%%%
\section{Conditional expectation}

We start with the definition of the expected value of a random variable $X$ with respect to a sub-$\sigma$-field $\mathcal B$, denoted $E^{\mathcal A}X$. The basic idea is to define $E^{\mathcal A}X$ as a Radon-Nikodym derivative. We then use this to construct $E(X|Y)$ and $E(X|Y=y)$. Play close attention to the fact $E(X|Y=y)$ is only unique up to a modification on $PY^{-1}$-null sets in $y$. After we define conditional probability distributions, namely $\mathcal L_{X|Y = y}$, we will show that once can construct a special version of $E(X|Y=y)$ that has nice properties.

{\em Remark:} Previously in the notes we have used notation such as $X$ or $Y$ to denote random variables (or vectors). In particular, measurable maps defined on probability space which map  into $\Bbb R$ (or $\Bbb R^d$). In this section we will slightly depart from that notational convention and generally write $X$ and $Y$, etc. for extended random variables.

\subsection{Definition of $E^{\mathcal A}(X)$}
\begin{theorem}[{\bf Construction of $E^{\mathcal A }X$}]
Let $(\Omega, \mathcal F, P)$ be a probability space and $X$ be a $P$-quasi-integrable extended random variable on $\Omega$. Let $\mathcal A\subset \mathcal F$ be a sub $\sigma$-field. Then there exists a
$\mathcal A$-measurable, $P$-quasi-integrable extended random variable $E^{\mathcal A}(X)$
such that
\begin{equation}
\label{eq: smooth condition}
{\int_A X\, dP = \int_A E^{\mathcal A}(X) \, dP\quad\text{for all $A\in \mathcal A$.}}
\end{equation}
Moreover $E^{\mathcal A}X$ is $P$-unique.
\end{theorem}


\begin{proof}
My game plan is to show the result for non-negative random variables then define the general case with $E^{\mathcal A}(X^+) - E^{\mathcal A}(X^-)$.

Start by assuming $X\geq 0$. Notice that
\[
\nu_{\mathcal A}(A) := \int_A X\, dP
\]
is a measure over $\mathcal A$. Let $P_{\mathcal A}$ denote the restriction of $P$ to the sub $\sigma$-field $\mathcal A$. Now we clearly have   $\nu_{\mathcal A} \lll P_{\mathcal A}$ over $\mathcal A$.  The Radon-Nikodym Theorem \ref{thm: rn2} gives that $d\nu_{\mathcal A}/dP_{\mathcal A}$ exists and is $\mathcal A$-measurable and $P$-quasi-integrable. Now
\[
E^{\mathcal A}(X) := \frac{d\nu_{\mathcal A}}{dP_{\mathcal A}}
\]
 has all the required properties. In particular $E^{\mathcal A}X$ is $\mathcal A$-measurable, $P$-quasi-integrable and for all $A\in \mathcal A$ we have
\begin{align*}
\int_A XdP & = \nu_{\mathcal A}(A) =\int_A E^{\mathcal A}(X) dP_{\mathcal A}=\int_A E^{\mathcal A}(X) dP
\end{align*}
where the last equation  follows by a change-of-variables Theorem \ref{thm: change of variables} (setting $T$ to be the identity map).


To extend to all $P$-quasi-integrable  random variables $X$ we need to ensure that $E^{\mathcal A}(X^+) - E^{\mathcal A}(X^-)$ is defined when $X$ is $P$-quasi-integrable. To this end suppose {\sl wlog} that $E(X^+)<\infty$. We show  $E^{\mathcal A}(X^+)<\infty$. In this case $\nu_{\mathcal A} := \int_\bullet X^+\, dP $ is a finite measure which implies (by item \ref{item: finite rn} in Theorem \ref{thm: props of rn}) that $E^{\mathcal A}(X^+):= \frac{d\nu_{\mathcal A}}{dP_{\mathcal A}}$ is $P$-integrable and therefore finite $P$-a.e. Therefore, we may, and do, change $E^{\mathcal A}(X^+)$ on a $P$-null set, without destroying condition (\ref{eq: smooth condition}),  so that $E^{\mathcal A}(X^+)<\infty$ which ensures $E^{\mathcal A}(X^+) - E^{\mathcal A}(X^-)$ is defined and, since $E^{\mathcal A}(X^+)\in L_1(P)$, $E^{\mathcal A}(X^+) - E^{\mathcal A}(X^-)$ is quasi-integrable and has the right integration properties.

Uniqueness follows directly from the {\em uniqueness of densities} Theorem \ref{thm: uniqueness of densities} since any $\mathcal A$-measurable, $P$-quasi-integrable $E^{\mathcal A}X$ which satisfies the right-hand-side of (\ref{eq: smooth condition}) is $P$-unique (to apply that theorem I'm using the fact that the base measure $P$ is $\sigma$-finite).
\end{proof}

\begin{example}[{\bf Smoothing property of $E^{\mathcal A}X$}] For example work with $([0,1), \mathcal B^{[0,1)}, \mathcal L)$. Let $\mathcal A :=\{[0,1), \varnothing, [0,1/2), [1/2,1) \}$. Show that
\[
[E^{\mathcal A}X](\omega) = \begin{cases}
\text{average of $X$ over $[0,1/2)$} & \text{if $\omega\in [0,1/2)$} \\
\text{average of $X$ over $[1/2,1)$} & \text{if $\omega\in [1/2,1)$}.
\end{cases}
\]
\end{example}

\begin{example}[{\bf Resolution of $E^{\mathcal A}X$ as expressing information}]
A nice heuristic for understanding how $\left(E^{\mathcal A}X\right)(\omega)$ is expressing partial information is that one can think of $\left(E^{\mathcal A}X\right)(\omega)$ as the average value of $X$ (wrt measure $P$) over the smallest event in $\mathcal A$ containing $\omega$ (however, this only holds rigorously when the $\sigma$-field is generated by a countable partition of $\Omega$). The smaller the smallest event it is, the more information/resolution you have.

In particular let  $\mathcal F_0 \subset \mathcal F_1 \subset \cdots \mathcal F$ be an increasing sequence of sub $\sigma$-fields where lets set $\mathcal F_0:= \{\Omega, \varnothing  \}$.
 Then the corresponding conditional expected values has increasing resolution from no resolution at all, i.e. $E(X)$, to full resolution, i.e. $X$
\[
\begin{array}{l}
E^{\mathcal F_0}(X) = E(X) \\
E^{\mathcal F_1}(X) \\
\quad\left\downarrow\vphantom{\displaystyle\int}\right.\text{\small increasing resolution} \\
%\quad\longdownarrow\text{\tiny increasing resolution} \\
E^{\mathcal F}(X) = X.\vphantom{\displaystyle\int}\\
\end{array}
\]
\end{example}

\begin{example}[{\bf Viewing $E^{\mathcal A}(X)$ as a projection}]
Another way to look at $E^{\mathcal A}(X)$ is with projection. This only works when $X\in L^2(P)$. Let $S$ denote the subset of $L_2(P)$ which are $\mathcal A$-measurable. It's easy to see that $S$ is a closed linear subspace of $L_2(P)$. Then we can project $X$ onto $S$, denoted $\mathcal P_SX$, which has the property
\[
(X - \mathcal P_SX) \perp W
\]
for all $W\in S$. Therefore $E[(X - \mathcal P_SX)W]=0$ for all $W\in S$. Therefore $E[XW]=E[(\mathcal P_SX)W$ for all $W\in S$. Substatuting $W= I_A$ in the last equation for some $A\in \mathcal A$ gives
\[
\int_A X dP = \int_A \mathcal P_SX dP.
\]
This shows that $\mathcal P_SX$ serves as $E^{\mathcal A}X$.
\end{example}



\begin{theorem}[{\bf Smoothing properties of $E^{\mathcal A}$}]
Let $(\Omega, \mathcal F, P)$ be a probability space and $\mathcal A_1, \mathcal A_2$ be sub $\sigma$-fields of $\mathcal F$. Suppose that $Y, X$ are $P$-quasi-integrable extended random variables on $(\Omega, \mathcal F, P)$.
Then
\begin{enumerate}
\item ${E(E^{\mathcal A}X) =_{a.e.} E(X)}$
\item If $\mathcal A_1\subset \mathcal A_2$ then $E^{\mathcal A_1}(E^{\mathcal A_2}X) =_{a.e.} E^{\mathcal A_2}(E^{\mathcal A_1}X) =_{a.e.} E^{\mathcal A_1}X$.
\item $E^{\mathcal A}X\in Q^{\pm}(P) \Longleftrightarrow X\in Q^\pm(P)$
\item If $XY\in Q(P)$ and  $X$ is $\mathcal A$-measurable (but not necessarily in $Q(P)$) then $E^{\mathcal A}(XY) =_{a.e} X E^{\mathcal A}Y$.
\end{enumerate}
\end{theorem}


\begin{theorem}[{\bf Expected value properties of $E^{\mathcal A}$}] Let $(\Omega, \mathcal F, P)$ be a probability space and $\mathcal A$ be a sub $\sigma$-field of $\mathcal F$.
Suppose that $Y, X, X_1, X_2$ are extended random variables on $(\Omega, \mathcal F, P)$.
Then
\begin{enumerate}
\item {\bf Monotonicity:} If $X,Y \in Q(P)$ then
\[
{\text{$X\leq Y$ $P$-a.e.}\Longrightarrow \text{$E^{\mathcal A} (X) \leq E^{\mathcal A} (Y)$ $P$-a.e.}}  \]
\item {\bf Linearity:}
If $X\in Q(P)$ and $\alpha \in \Bbb R$ or $X\in \mathscr N$ and $\alpha\in\{\infty, -\infty \}$ then $\alpha X\in Q(P)$
\[
E^{\mathcal A} (\alpha X) =_{a.e.}  \alpha E^{\mathcal A} (X)
\]
If $X, Y, X+Y\in Q(P)$ then
\[
I_A E^{\mathcal A} (X +  Y) =_{a.e.}  I_A E^{\mathcal A} (X) + I_A E^{\mathcal A} (Y)
\]
where $A:=\{E^{\mathcal A} (X) + E^{\mathcal A} (Y)\neq \pm \infty \mp\infty\}$.
\item {\bf Continuous from below}:
\[
{\text{$0\leq X_n\uparrow X$ $P$-a.e.}\Longrightarrow E^{\mathcal A} (X_n) \uparrow E^{\mathcal A} (X)\,\, \text{$P$-a.e.} }
 \]
\item {\bf Fatou:} If  $X_n\geq 0$ a.e. then
\[
{
E^{\mathcal A}( \liminf_{n\rightarrow \infty} X_n ) \leq \liminf_{n\rightarrow \infty} E^{\mathcal A}(X_n)\,\, \text{$P$-a.e.}
}
\]
\item {\bf DCT:} If $X_n, X\in Q(P)$ and  $X_n\aerightarrow X$ then
\[
{
\lim_{n\rightarrow \infty} E^{\mathcal A}(X_n) = E^{\mathcal A}(X) \,\, \text{P-a.e. on  $\{E^{\mathcal A}(\sup_n |X_n|)<\infty \}$}.
}
\]
\end{enumerate}
\end{theorem}



\subsection{Defining $E(X|Y)$ and $E(X|Y=y)$}


\begin{definition}
Suppose $X$ is an extended random variable on $(\Omega, \mathcal F, P)$. Suppose $(\mathcal Y, \mathcal F^{\mathcal Y})$ is another measurable space and
$Y:\Omega \rightarrow \mathcal Y$ which is $\mathcal F/\mathcal F^{\mathcal Y}$ measurable. Then
\[
E(X|Y) := E^{\,\sigma\langle Y \rangle} X
\]
\end{definition}

\begin{corollary}[{\bf $E(X|Y)$ is a function of $Y$}]
There exists a $\mathcal F^{\mathcal Y}$-measurable function $g\colon \mathcal Y \rightarrow \bar{\Bbb R}$ such that $E(X|Y)(\omega) = g(Y(\omega))$ for all $\omega \in \Omega$.
\end{corollary}
\begin{proof}
This follows directly from Corollary \ref{cor: funs of measurable funs} since, by definition, $E(X|Y) = E^{\,\sigma\langle Y \rangle} X$ is measurable with respect to $\sigma\langle Y \rangle$.
\end{proof}


At times we use the notation $E(X|Y=y)$, for $y\in \mathcal Y$, to denote $g(y)$ where $E(X|Y)(\omega) = g(Y(\omega))$. However, since $g$ can be modified on $P$ on $PY^{-1}$ null sets of $y$'s is not meaningful to talk about $E(X|Y=y)$ at a fixed $y$, but rather about how $E(X|Y=y)$ integrates over $y$.



\begin{corollary}[{\bf Some obvious properties}]
Let $X$ be an  extended random variable on the probabilist space $(\Omega, \mathcal F, P)$. Let $(\mathcal Y, \mathcal F^{\mathcal Y})$ be a measure space. Let $Y:\Omega \rightarrow \mathcal Y$ be $\mathcal F/\mathcal F^{\mathcal Y}$ measurable. Then
\begin{enumerate}
\item ${E(X) =_{a.e.} E(E(X|Y))}$;
\item ${E(f(Y)X|Y)) =_{a.e.} f(Y)E(X|Y)}$ whenever $f(y): \mathcal Y \rightarrow \bar{\Bbb R}$ is $\mathcal F^{\mathcal Y}$-measurable and $Xf(Y)\in Q(P)$.
\end{enumerate}
\end{corollary}



\begin{exercise}
Let $\mathcal P$ be a $\pi$-system generating the sub-$\sigma$-field $\mathcal A$ of $\mathcal F$ such that $\Omega\in\mathcal P$, and let $X:\Omega \rightarrow [0,\infty]$ be $\mathcal F$-measurable and $P$-integrable function. Suppose also that $Y$ is $\mathcal A$-measurable  $P$-integrable function such that $\int_{A} X\, dP = \int_{A} Y\, dP$ for all $A\in \mathcal P$. Show that $Y$ is a version of $E^{\mathcal A} X$.
\end{exercise}

\begin{exercise}
Let $\mathcal A_1$ and $\mathcal A_2$ be sub-$\sigma$-fields of $\mathcal F$ and for $i = 1,2$ let $\mathcal X_i$ be the collection of $\mathcal A_i$-measurable mappings from $\Omega$ to $[0,\infty]$. Notice that $\mathcal A_1$ and $\mathcal A_2$ are {\bf independent}, written $\mathcal A_1 \perp \mathcal A_2$ if and only if $E(X_1X_2)=E(X_1)E(X_2)$ for all $X_1\in \mathcal X_1$ and $X_2\in \mathcal X_2$. $\mathcal A_1$ and $\mathcal A_2$ are said to be {\bf conditionally independent} given a sub-$\sigma$-field $\mathcal B$ of $\mathcal F$, written $\mathcal A_1 \perp_{\mathcal B} \mathcal A_2$, if and only if
\[
E^{\mathcal B}(X_1X_2) = E^{\mathcal B}(X_1) E^{\mathcal B}(X_2) \text{ for all $X_1\in\mathcal X_1$, $X_2\in\mathcal X_2$}.
\]
Also let $\mathcal C \vee \mathcal D$ denote $\sigma\langle \mathcal C, \mathcal D\rangle$ when $\mathcal C$ and $\mathcal D$ are two collections of events on $\Omega$.
Show that
\begin{enumerate}
\item  $\mathcal A_1 \perp_{\mathcal B} \mathcal A_2$ $\Longleftrightarrow$ $E^{\mathcal B}(I_{A_1}I_{A_2}) = E^{\mathcal B}(I_{A_1})E^{\mathcal B}(I_{A_2}) $ a.e.\! for all $A_1\in \mathcal A_1$ and $A_2\in \mathcal A_2$.
\item $\mathcal A_1 \perp_{\mathcal B} \mathcal A_2$ $\Longleftrightarrow$ $E^{\mathcal A_1 \vee \mathcal B}X_2 = E^{\mathcal B} X_2$ a.e.\! for all $X_2\in\mathcal X_2$.
\item $\mathcal A_1 \perp \mathcal A_2$ $\Longleftrightarrow$ $E^{\mathcal A_1 }X_2 = E X_2$ a.e.\! for all $X_2\in\mathcal X_2$.
\item $\mathcal A_1 \perp_{\mathcal B} \mathcal A_2$ $\Longleftrightarrow$ $( \mathcal A_1 \vee \mathcal B)\perp_{\mathcal B} (\mathcal A_2\vee \mathcal B)$.
\item $(\mathcal A_1 \vee \mathcal B) \perp \mathcal A_2$ $\Longleftrightarrow$ $\mathcal B\perp \mathcal A_2$ and $\mathcal A_1 \perp_{\mathcal B} \mathcal A_2$.
\end{enumerate}
Hint for {\small 2.}\! ($\Longrightarrow$), it suffices (why?) to consider the case where $X_2$ is integrable; apply the preceding exercise with $X=X_2$ and $\mathcal P = \{A_1\cap B\colon A_1\in \mathcal A_1 \text{ and } B\in \mathcal B \}$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%
\subsection{The substitution fallacy}
{\bf The substitution fallacy:}
\begin{quote}
 \it Let $(\Omega, \mathcal F, P)$  be a probability space.
Let $(\mathcal X, \mathcal F^{\mathcal X})$ and $(\mathcal Y, \mathcal F^{\mathcal Y})$ be two other measurable spaces. Let $X\colon \Omega\rightarrow \mathcal X$ and $Y\colon \Omega \rightarrow \mathcal Y$ be measurable maps into their respective measurable spaces.
 Let $f(x,y): \mathcal X \times \mathcal Y \rightarrow \bar{\Bbb R}$ be $\mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y}$-measurable and quasi-integrable with respect to $P(X,Y)^{-1}$. Then
\begin{equation}
\label{sub fallacy}
E(f(X,Y)|Y=y) = E(f(X,y)|Y=y)
\end{equation}
\end{quote}

The problem with the above statement is that it is not clear what is meant by equation (\ref{sub fallacy}). The left hand side is $g(y)$ where $g(Y) = E(f(X,Y)|Y) = E^{\, \sigma\langle Y\rangle} f(X,Y)$ is a function of $\Omega$. Now what do we mean by the right hand side of (\ref{sub fallacy})? If we fix $y$, maybe we consider $f(X,y)$ to be a function on $\Omega$. Then  there exists $g_y \colon \mathcal Y \rightarrow \bar{\Bbb R}$ such that $g_y(Y)= E(f(X,y) | Y)$. In this case we are asking if $g(y)=g_y(y)$. One can immediately see the problem. The functions $g_y$ is $PY^{-1}$ unique. So if, say, that $P(Y=y)=0$ for every $y$, then I can change $g_y$ at $y$ to be any number I want and not destroy the fact that $g_y(Y)$ would serve as a version of $E(f(X,y) | Y)$. This implies that $g_y(y)$ is not well defined.


\begin{theorem}[{\bf A correct version of substitution}]
If,  in addition to the antecedent presented in the substitution fallacy, $X$ and $Y$ are independent, then $E[f(X,y)]$ serves as a version of $E(f(X,Y)|Y=y)$.
\end{theorem}

A complete resolution of the substitution fallacy can not be resolved until the next section when we talk about regular conditional probability distributions


\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section
%
%%%%%%%%%%%%%%%%%%%%%%
\section{Conditional probability}

The main story is that $P(A|B)$ doesn't have any meaning when $P(B)=0$. We can only make sense of this when $B$ has the form $Y=y$ for some $y$. However, there may be multiple different random variables, say $Y_1$ and $Y_2$, such that $\{Y_1 = y_1\} = \{Y_2 = y_2\} =  B$ but $P(A|Y_1 = y_1)\neq P(A|Y_2 = y_2)$. So, in effect, what we mean by $P(A|B)$ depends on what which random quantity $Y$ we choose.

\begin{definition}[{\bf Probability of $A$ given $Y$ or $\mathcal A$}]
Let $(\Omega, \mathcal F, P)$  and $(\mathcal Y, \mathcal F^{\mathcal Y})$ be a probability space and measure space, respectively. Let $Y:\Omega \rightarrow \mathcal Y$ be $\mcirc \mathcal F/ \mathcal F^{\mathcal Y}$ and $\mathcal A\subset \mathcal F$ be a sub-$\sigma$-field. Let $F\in \mathcal F$ and define
\begin{align*}
P(F|\mathcal A)&:= E^{\mathcal A}(I_F) \\
P(F|Y)&:= E(I_F |Y)\\
P(F|Y=\bullet)&:= E(I_F |Y=\bullet)
\end{align*}
\end{definition}


Let's step back and unwind the definition a bit. First notice that by the definition of $E(I_F|Y)$ we have that
\[P\bigl(A\cap F\bigr) = \int_A P\bigl(F | Y \bigr) dP \]
for all $A\in \sigma \langle Y\rangle $. Since every $A\in\sigma \langle Y\rangle = Y^{-1}(\mathcal F^{\mathcal Y})$ has the form $Y^{-1}(B)$ for some $B\in  \mathcal F^{\mathcal Y}$,  one can write $A = \{Y\in B \}$ and  therefore
\begin{align*}
P\bigl( \{Y\in B \}\cap F\bigr)
& = \int_{\{Y\in B \} } P(F | Y ) dP \\
&= \int_{\Omega} I_{\{Y\in B \} } P(F | Y ) dP  \\
&= \int_{\Omega} I_{B}(y) P(F | Y= y ) dPY^{-1} (y) \\
&= \int_{B} P(F | Y= y ) dPY^{-1} (y)
\end{align*}
which is what one might call the law of total probability.
Notice that in the case that $F$ corresponds to the event $\{X\in A \}$ for some extended random variable on $(\Omega, \mathcal F, P)$, then the above equation simplifies to
\[
P\bigl(Y \in B , X\in A \bigr)
= \int_{B} P\bigl(X\in A | Y= y \bigr) dPY^{-1} (y)
\]

At this point it would seem like there is nothing else to do. I've defined the conditional probability of $F$ or $\{ X\in A \}$ given $Y = y$. But, notice we still don't know that if we fix $y$, that $P(F|Y=y)$ is a genuine probability measure when we let $F$ vary over $\mathcal F$. The problem is that if we set $P(F |Y)= g_F(Y)$, then we may not necessarily have that $g_F(y)$ is a probability measure on $(\Omega, \mathcal F)$ for each fixed $y$. The problem is that I'm free to change $g_F(y)$ on a $PY^{-1}$ null set of $y$'s for each fixed $F$. What we want is to find a version of $g_F(y)$ for each $F\in\mathcal F$ that satisfies the following definition.

\begin{definition}[{\bf Conditional probability distribution}]
\label{df: cpd}
Let $(\Omega, \mathcal F, P)$ be a probability space,  $(\mathcal Y, \mathcal F^{\mathcal Y})$ be a measurable space and $Y\colon \Omega \rightarrow \mathcal Y$ be $\mcirc \mathcal F/\mathcal F^{\mathcal Y}$.
A map $g(F|y)\colon \mathcal F\times \mathcal Y \rightarrow [0,1]$ is a {\bf conditional probability distribution on $(\Omega, \mathcal F)$ given $Y$}, if
\begin{enumerate}
\item\label{item one for cpd} For  every $y\in \mathcal Y$, the function  $g(\,\bullet\,| y )$ is a probability measure on $\mathcal F$;
\item \label{item two for cpd} For every $F\in\mathcal F$, the function $g(F|\,\bullet\, )$ is $\mcirc\mathcal F^{\mathcal Y}/\mathcal B^{\bar{\Bbb R}}$, $PY^{-1}$-quasi-integrable and
\[P( A\cap F) = \int_A g(F|y) dPY^{-1}(y) \]
for all $A\in \sigma\langle Y\rangle$.
\end{enumerate}
\end{definition}

We will mainly be interested in conditional probability distributions of a random $X$ with respect to some random $Y$. This is technically subsumed by the previous definition by setting $\{X \in A \} = F = \mathcal  F$ but it will be more clear if we define a specific definition.

\begin{definition}[{\bf Conditional probability distribution of $X$ given $Y=y$}]
Let $(\Omega, \mathcal F, P)$ be a probability space. Let $(\mathcal X, \mathcal F^{\mathcal X})$ and $(\mathcal Y, \mathcal F^{\mathcal Y})$ be two measurable spaces. Let $X\colon \Omega \rightarrow\mathcal X$ and $Y\colon \Omega\rightarrow \mathcal Y$ be two functions which are $\mcirc \mathcal F/\mathcal F^{\mathcal X}$ and $\mcirc \mathcal F/\mathcal F^{\mathcal Y}$ respectively.
A map $\mathcal L_{X|Y=y}(A)\colon  \mathcal F^{\mathcal X} \times \mathcal Y \rightarrow [0,1]$ is a {\bf conditional probability distribution of $X$ given $Y$}, if
\begin{enumerate}
\item For each $y\in\mathcal Y$, the function $\mathcal L_{X|Y=y}(\bullet)$ is a probability measure on $\mathcal F^{\mathcal X} $.
\item For each $A \in \mathcal F^{\mathcal X}$, the function $\mathcal L_{X|Y=\bullet}(A)$ is $\mcirc \mathcal F^{\mathcal Y}/\mathcal B^{\bar{\Bbb R}}$, $PY^{-1}$-quasi-integrable and
\[ P\bigl( Y\in B ,\, X\in A\bigr) = \int_B \mathcal L_{X|Y=y}(A) \,dPY^{-1}(y) \]
for all $B\in \mathcal F^{\mathcal Y}$.
\end{enumerate}
\end{definition}


So, the main result of this section is to figure out conditions that allow us to construct a version of  $E(I_F|Y = y)$, for each $F$, which forms  {\it conditional probability distribution}. Before we get to existence theorems lets sharpen our intuition on some other theorems (non-existence, uniqueness, the density case, the independence case, the infinite dimensional case)

{\bf How to think about a conditional probability distribution:}
Remember that  it doesn't really mean much to talk about $E(X|Y=y)$ for a fixed $y$ when $P[Y=y]=0$, since $E(X|Y)$  is of the form  $g(Y)$ and one is free to change $g$ on a null set of $y$'s. This will continue to be true even , when later, we will show that there is often a particular choice of $E(X|Y=y)$ which has nice properties (like the substitution principle can be proven). The fact remains $E(X|Y=y)$ is still only meaningful to talk about how it integrates over $y$, not the value at any particular $y$.

The same message will hold true for $\mathcal L_{X|Y=y}$.  At any fixed $y$ when $P[Y=y]=0$, $\mathcal L_{X|Y=y}$ has no real meaning. The example presented in the introduction serves as a perfect example of this. Indeed, if you let $Z := X/Y$ and $W := X-Y $, then the results in the next section show that $\mathcal L_{X|Z=z}$ and $\mathcal L_{X|W=w}$ both exist as regular conditional probability distributions for $X$ given $Z$ and for $X$ given $W$, respectively. Now, the events $W = 0$ and $Z = 1$ are exactly the same as subsets of $\Omega$. However, $\mathcal L_{X|Z=1}[B]\neq \mathcal L_{X|W = 0}[B]$. There is no contradiction here, except for the fallacy that  $P[X\in B| Z=1  ]$ or $P[X\in B| W=0  ]$ means anything. Rather we can only really make sense about how $P[X\in B| Z=z ]$ integrates as a function of $z$ and similarly for $P[X\in B| W=w]$.

One thing to notice, and this might clear things up a bit, is that
\[
\lim_{\epsilon\rightarrow 0}P[X\in B| Z\in B_1^\epsilon ]\neq \lim_{\epsilon\rightarrow 0} P[X\in B| W\in B_0^\epsilon ]
\]
 where $B_x^\epsilon := \{y\in\Bbb R\colon |x-y|<\epsilon  \}$ is the open ball, around $x$, of radius $\epsilon$. This is one way to make sense of why $\mathcal L_{X|Z=1}[B]$ and $\mathcal L{X|W = 0}[B]$ are different. However, in this example, there is a sense in which $\mathcal L_{X|Z=z}$ and $\mathcal L_{X|W = w}$ are continuous. Therefore the fact that the limits are different is just a manifestation of the fact that they integrate differently over some $z$ and $w$ regions.  See theorem \ref{limiting construction} for reference to the continuity result.


%%%%%%%%%% subsection
\subsection{Uniquness, density case, etcetra}


\begin{sectionassumption}
For the remainder of this section, unless otherwise stated, we make with the following assumptions:
Let $(\Omega, \mathcal F, P)$ be a probability space. Let $(\mathcal X, \mathcal F^{\mathcal X})$ and $(\mathcal Y, \mathcal F^{\mathcal Y})$ be two measurable spaces. Let $X\colon \Omega \rightarrow\mathcal X$ and $Y\colon \Omega\rightarrow \mathcal Y$ be two functions which are $\mcirc \mathcal F/\mathcal F^{\mathcal X}$ and $\mcirc \mathcal F/\mathcal F^{\mathcal Y}$ respectively.
\end{sectionassumption}

\begin{theorem}[{\bf Sometimes a cpd does not exist}]
\end{theorem}


\begin{theorem}[{\bf Factor the joint}]
\label{thm: factor the joint}
Let $\mathcal L_{X,Y}$ be the joint law of $(X,Y)$ (i.e. $P(X,Y)^{-1}$) and $\mathcal L_Y$ be the  marginal law of $Y$ (i.e. $PY^{-1}$). Let $\mathcal L_{X|Y=y}$ be a conditional probability distribution of $X$ given $Y$. Then for any $F\in \mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y}$ the function $\mathcal L_{X|Y=y}(F_y)$ is $\mathcal L_Y$-quasi-integrable (where $F_y:=\{x\in\mathcal X\colon (x,y)\in F$) and
\begin{equation}
\label{eq: factor the joint}
\mathcal L_{X,Y}(F) = \int_{\mathcal Y} \mathcal L_{X|Y=y}(F_y) \,d\mathcal L_Y(y).
\end{equation}
\end{theorem}
\begin{proof}
Notice that $F_y\in\mathcal F^{\mathcal X}$ for each $y\in\mathcal Y$ by Theorem \ref{thm: sections are measurable} so that $\mathcal L_{X|Y=y}[F_y]$ is well defined. Now the results follows from the following three parts of proof.

({\sl Part  I: $\mathcal L_{X|Y=y}[F_y]$ is $\mathcal L_Y$-quasi-integrable}) Since $\mathcal L_{X|Y=y}[F_y]$ is required to take values in $[0,1]$ it is sufficient to show $\mathcal F^{\mathcal Y}/\mathcal B^{\bar{\Bbb R}}$-measurablilty.  We use good sets. Define
\begin{equation}
\mathcal G:=\bigl\{F\in  \mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y}\colon \text{ $\mathcal L_{X|Y=y}[F_y]$ is $\mcirc\mathcal F^{\mathcal Y}/\mathcal B^{\bar{\Bbb R}}$} \bigr\}.
\end{equation}
\begin{itemize}
\item
 The measurable rectangles are in $\mathcal G$ since $\mathcal L_{X|Y=y}[(B\times A)_y]= I_{A}(y)\mathcal L_{X|Y=y}[B]$ and $\mathcal L_{X|Y=y}[B]$ is required to be $\mcirc\mathcal F^{\mathcal Y}/\mathcal B^{\bar{\Bbb R}}$.
 \item  $\mathcal G$ is also closed under complementation. In particular, if $F\in\mathcal G$ then
\[ \mathcal L_{X|Y=y}[(F^c)_y] = \mathcal L_{X|Y=y}[(F_y)^c] = 1- \mathcal L_{X|Y=y}[F_y]  \]
which measurable by the closure theorem.
\item
Finally notice that  $\mathcal G$ is closed under disjoint union. In particular, suppose $F_1, F_2, \ldots\in\mathcal G$ are all disjoint. Then
\[  \mathcal L_{X|Y=y}[(\cup_n F_n)_y] = \mathcal L_{X|Y=y}[\cup_n (F_n)_y] = \textstyle\sum_n\mathcal L_{X|Y=y}[F_n] \] which is measurable by the closure theorem.
\end{itemize}
The above bullets show that $\mathcal G$ is a $\lambda$-system which is generated by the $\pi$ system of measurable rectangles. Therefore
\[
\mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y} = \sigma\langle \text{rectangles} \rangle = \lambda\langle\text{rectangles} \rangle\subset\mathcal G
\]
where the second `$=$' follows from Dynkin's $\pi-\lambda$ theorem and `$\subset$' follows from good sets. Therefore
$\mathcal L_{X|Y=y}[F_y]$ is $\mcirc\mathcal F^{\mathcal Y}/\mathcal B^{\bar{\Bbb R}}$ for every $F\in \mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y} $.



({\sl Part  II: RHS of (\ref{eq: factor the joint}) is a probability measure}) This follows since slicing commutes with set operations, $\mathcal L_{X|Y=y}$ is a probability measures for each $y$, and by properties of the integral $\int_{\mathcal Y} \bullet \,d\mathcal L_Y(y)$, in particular Theorem \ref{thm: Integration term by term}.

({\sl Part  III: (\ref{eq: factor the joint}) holds on rectangles})
Let $B\in \mathcal F^{\mathcal X}$ and $A\in \mathcal F^{\mathcal Y}$ so that $B\times A \in \mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y}$. Notice that $(B\times A)_y = B$ when $y\in A$ and $(B\times A)_y = \varnothing$ when $y\notin A$. Therefore
\begin{align*}
\int_{\mathcal Y} \mathcal L_{X|Y=y}&[(A\times B)_y] \,d\mathcal L_Y(y) \\
& = \int_{A} \mathcal L_{X|Y=y}[B] \,d\mathcal L_Y(y)\\
& = P[Y\in A, X\in B],\,\text{ property of $\mathcal L_{X|Y=y}$}\\
& = \mathcal L_{X,Y}[A\times B].
\end{align*}

Therefore  (\ref{eq: factor the joint}) holds on the $\pi$-system of measurable rectangles which generates $\mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y}$. By uniqueness of probability measures on $\pi$-system generators we have  (\ref{eq: factor the joint}) holds on all of $\mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y}$.

\end{proof}


\begin{theorem}[{\bf Uniqueness of cpd}]
 Let $g(F|y)$ and $g^*(F|y)$ be two conditional probability distributions on $(\Omega, \mathcal F)$ given $Y$.
If $\mathcal F$ is countably generated then
\begin{equation}
\label{uniq cpd equation}
P\Bigl(\text{$g(F |Y) = g^*(F | Y)$ for all $F\in\mathcal F$}\Bigr) = 1.
\end{equation}
\end{theorem}
\begin{proof}
Let $\mathcal F_0$ be  a countable set of generators such that $\mathcal F = \sigma\langle\mathcal F_0\rangle$. Notice that we can suppose without loss of generality that $\mathcal F_0$ is also a $\pi$-system (by closing $\mathcal F_0$ under finite intersection, which preserves countability). Since measures are equal if they agree on a $\pi$-system generating set we have
\begin{align}
\bigl\{&\text{$g(F |Y) = g^*(F | Y)$ for all $F\in\mathcal F$}\bigr\}\nonumber \\
&\qquad\qquad=\bigl\{ \text{$g(F |Y) = g^*(F | Y)$ for all $F\in\mathcal F_0$} \bigr\}\nonumber \\
&\qquad\qquad=\bigcap_{F\in\mathcal F_0}\{ g(F |Y) = g^*(F | Y) \}. \label{showme 245}
\end{align}
Notice that for each $F\in\mathcal F_0$, $g(F|\,\cdot\,)$ and $g^*(F|\,\cdot\,)$ are $\mcirc \mathcal F^{\mathcal Y}/\mathcal B^{\bar{\Bbb R}}$.
Therefore $g(F|Y(\,\cdot\,))$ and $g^*(F|Y(\,\cdot\,))$ is $\mcirc \mathcal F/\mathcal B^{\bar{\Bbb R}}$ (by Theorem \ref{thm: composition of measurable}) and hence $\{\omega\in\Omega \colon  g(F |Y(\omega)) - g^*(F | Y(\omega))=0\}$ is an $\mathcal F$-measurable set. Therefore  $\{\text{$g(F |Y) = g^*(F | Y)$ for all $F\in\mathcal F$}\}$ is an $\mathcal F$-measurable set.

Now the theorem follows by noticing that for any fixed $F\in\mathcal F$ $g(F|Y)$ and $g^*(F|Y)$ are both a version of $E(I_F|Y)$, which is unique $PY^{-1}$ almost everywhere. Therefore $P\bigl(g(F |Y) = g^*(F | Y) \bigr)=1$ and hence, by (\ref{showme 245}), (\ref{uniq cpd equation}) follows.
\end{proof}



\begin{theorem}[{\bf The density case}]
Let $\mathcal L_{X,Y}$ denote the joint distribution of $X$ and $Y$ on $(\mathcal X\times \mathcal Y, \mathcal F^X\otimes \mathcal F^Y)$ (see Theorem \ref{thm: clump} for existence). Suppose  $\mu$ and $\sigma$ are $\sigma$-finite measures on $(\mathcal X, \mathcal F^{\mathcal X})$  and $(\mathcal Y, \mathcal F^{\mathcal Y})$ respectively. If $f(x,y) $  is a density of $\mathcal L_{X,Y}$ with respect to $\mu \otimes \sigma$ on $(\mathcal X\times \mathcal Y, \mathcal F^{\mathcal X}\otimes\mathcal F^{\mathcal Y})$ then for each $y\in\mathcal Y$ define
\[
f_{X|Y = y}(x) :=
\begin{cases}
 f(x,y) / f_Y(y), & \text{when $y \in G$}\\
 f_X(x), & \text{when $y \not\in G$}
\end{cases}
\]
where
\begin{align*}
f_Y(y)&:= \int_{\mathcal X} f(x,y) \,d\mu(x) \\
f_X(x)&:= \int_{\mathcal Y} f(x,y)\,d\sigma(y)
\end{align*}
 and $G:=\{y\colon 0< f_Y(y) <\infty \}$. Then
$\mathcal L_{X|Y=y}[F] := \int_F f_{X|Y = y}(x)\,d\mu(x)$ defines a cpd of $X$ given $Y$.
\end{theorem}
\begin{proof}
% By an extension of Corollary \ref{cor: integrate out the joint} one has the $f(y)$ is the density of the marginal distribution of $Y$ with respect to $\sigma$.

% For each $y$ define $\mathcal L_{X|Y=y}[F] = \int_F f_{X|Y = y}(x)\,d\mu(x)$ over $F\in \mathcal F^{\mathcal X}$. Let's check to make sure this integral is well defined. Since $f(y)$ is the marginal of $Y$, $f(y)$ is $\mathcal F^{\mathcal Y}$-measurable. Therefore $G$ is $\mathcal F^{\mathcal Y}$-measurable. Now $f(y)$ can be thought of $f(\pi_2(x,y))$ where $\pi_2(x,y) = y$ is the coordinated projection. This implies that $f(y)$ is $\mcirc \mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y}/\mathcal B^{\bar{\Bbb R}}$. This implies that $G\times \mathcal Y\in \mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y}$ and  $I_{G\times \mathcal Y}f(x,y) / f(y)$ is $\mcirc \mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y}/\mathcal B^{\bar{\Bbb R}}$.
% Therefore  $f_{X|Y = y}(x)$ is $\mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y}$. Since sectioning preserves measurability (see Theorem \ref{thm: sections are measurable}) and by the positivity of $f_{X|Y = y}$ we have that   $f_{X|Y = y}(\cdot)$ is quasi-integrable. Therefore $\mathcal L_{X|Y=y}$ is well defined.

By Fubini, $f_X$ and $f_Y$ are $\mcirc \mathcal F^{\mathcal X}$ and $\mcirc \mathcal F^{\mathcal Y}$, respectively, and can be modified on appropriate null sets to take values in $[0,\infty]$. Therefore
$f_{X|Y = y}(x)$ is $\mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y}$-measurable and non-negative.
For one thing, this immediately gives that $\mathcal L_{X|Y=y}$ is well defined.
Also notice that $P[Y\in G] = 1$. Indeed Fubini also gives that $f_Y$ and $f_X$ are the marginal densities of $Y$ and $X$, respectively, so that
\[ P[Y\in G^c] = \int_{f_Y=0} f_Y(y)d\sigma + \int_{f_Y=\infty} f_Y(y)d\sigma \leq 1.  \]
Obviously $ \int_{f_Y=0} f_Y(y)d\sigma = 0$ and $\int_{f_Y=\infty} f_Y(y)d\sigma$ must be zero or else it would violate the above upper bound.


({\sl Conditions for  $\mathcal L_{X|Y=y}[\bullet]$})
Since we already know that indefinite integrals of positive measurable functions are measures we just check  $\int_{\mathcal X} f_{X|Y = y}(x)\,d\mu(x)=1$, which follows easily by the definition of $f_{X|Y = y}$.

({\sl Conditions for $\mathcal L_{X|Y=\bullet}[F]$})
To prove the necessary conditions for $\mathcal L_{X|Y=\bullet}[F]$ notice that Fubini's theorem establishes $\int_F f_{X|Y = \bullet}(x)\,d\mu(x)$ is $\mathcal F^{\mathcal Y}$-measurable and quasi-integrable. Now notice that for each $A\in \mathcal F^{\mathcal Y}$ and $B\in \mathcal F^{\mathcal X}$ we have
\begin{align*}
P[&Y\in A, X\in B] \\
& = P[Y\in A\cap G, X\in B],  \quad \text{since $P[Y\in G] = 1$}\\
&= \int_{(A\cap G)\times B} f(x,y) \,d\mu\otimes \sigma \\
&= \int_{A\cap G} \left[\int_B f(x,y) \,d\mu(x)\right]  \,d\sigma(y) , \quad\text{Fubini} \\
&= \int_{A\cap G} \left[\int_B \frac{f(x,y)}{f_Y(y)}\,d\mu(x)\right]  f_Y(y)\,d\sigma(y) \\
&= \int_{A\cap G}  \mathcal L_{X|Y=y}[B] f_Y(y)\,d\sigma(y) \\
&= \int_{A\cap G}  \mathcal L_{X|Y=y}[B] dPY^{-1} \\
&= \int_{A} \mathcal L_{X|Y=y}[B] dPY^{-1}
\end{align*}
where the last line follows since $I_G=1$, $PY^{-1}$-a.e..
\end{proof}





\begin{theorem}[{\bf The independence case}]
If $X$ and $Y$ are independent then a conditional probability distribution of $X$ given $Y$ exists and a version of it is given by  $\mathcal L_{X|Y=y}= PX^{-1}$. Conversely if $\mathcal L_{X|Y = y} = Q$ for all $y\in \mathcal Y$ for some probability measure $Q$ on $(\mathcal X,\mathcal F^{\mathcal X})$, then $X$ and $Y$ are independent and $PX^{-1} = Q$.
\end{theorem}



\begin{theorem}[{\bf $\pi$-system tool}]
Let $\mathcal P$ be a $\pi$-system generating $\mathcal F^{\mathcal X}$. If $\{Q_y \}_{y\in\mathcal Y}$ is a collection of probabilty measures on $(\mathcal X,\mathcal F^{X})$ such that $Q_\bullet[F]$ is a version of $P[X\in F | Y=\bullet]$ for each $F\in \mathcal P$. Then a conditinoal probability distribution of $X$ given $Y$ exists and a version of it is given by $\mathcal L_{X|Y=y} = Q_y$.
\end{theorem}

\begin{proof}
Define $\mathcal G$ to be the set of all  $B\in\mathcal F^{\mathcal X}$ which satsifes:  {\it $Q_\bullet[B]$ is $\mathcal F^{\mathcal Y}$-measurable and $P[Y\in A, X\in B] = \int_A Q_y[B]\, dPY^{-1}(y) $ for all $A\in \mathcal F^{\mathcal Y}$}. Notice that $\mathcal G$ is a $\lambda$-system. By assumption $\mathcal G$ contains the $\pi$-system $\mathcal P$. By good sets $\lambda \langle \mathcal P \rangle \subset \mathcal G$. Then by Dynkin's $\pi\!-\!\lambda$ theorem we have $\lambda \langle \mathcal P \rangle = \sigma\langle \mathcal P \rangle = \mathcal F^{\mathcal X}\subset \mathcal G$.
\end{proof}



%%%%%%%%%% subsection
\subsection{Existance of $\mathcal L_{X|Y=y}$}

\begin{sectionassumption}
For the remainder of this section we continue with the following assumptions: Let  $(\Omega, \mathcal F, P)$ is  a probability space. Let $(\mathcal X, \mathcal F^{\mathcal X})$ and $(\mathcal Y, \mathcal F^{\mathcal Y})$ be two measurable spaces. Let $X\colon \Omega \rightarrow\mathcal X$ and $Y\colon \Omega\rightarrow \mathcal Y$ be two functions which are $\mcirc \mathcal F/\mathcal F^{\mathcal X}$ and $\mcirc \mathcal F/\mathcal F^{\mathcal Y}$ respectively.
\end{sectionassumption}


\begin{lemma}[{\bf Cumulative distributions give cpd's}]
Suppose $(\mathcal X,\mathcal F^{\mathcal X}) = (\mathcal R, \mathcal B^{\Bbb R})$ and there exists a function $F(\bullet | \bullet)\colon \mathcal R \times \mathcal Y\rightarrow \Bbb R$ such that
\begin{enumerate}
\item For each $y\in \mathcal Y$, $F(\bullet|y)$ is a cumulative distribution function;
\item For each $x \in \Bbb R$, $F(x | \bullet)$ is a version of $P[X\leq x | Y=\bullet]$.
\end{enumerate}
Then a conditional probability distribution of $X$ given $Y$ exists. Moreover any version of $\mathcal L_{X|Y=y}$ satisfies
\[\mathcal L_{X|Y=y}[X\leq x] = F(x|y)  \]
for each $y\in\mathcal Y$.
\end{lemma}



\begin{theorem}[{\bf Little existence theorem}]
If $(\mathcal X,\mathcal F^{\mathcal X}) = (\Bbb R, \mathcal B^{\Bbb R})$ then there exists a conditional probability distribution of $X$ given $Y$.
\end{theorem}



\begin{definition}[{\bf Isomorphic measure spaces}]
Two measurable spaces $(\Omega,\mathcal F)$ and $(\Omega^*,\mathcal F^*)$ are said to be {\bf isomorphic} if there exists a one-to-one mapping $\varphi$ from $\Omega$ onto $\Omega^*$ such that both $\varphi$ and $\varphi^{-1}$ are measurable.
\end{definition}


\begin{definition}[{\bf Standard Borel space}]
A measurable space $(\Omega, \mathcal F)$ is said to be a {\bf standard Borel space} if there exists a Borel set $B$ of $\Bbb R$ such that $(B, B\cap \mathcal B^{\Bbb R})$ is isomorphic to  $(\Omega, \mathcal F)$.
\end{definition}


\begin{theorem}[{\bf Big existence theorem}]
If $(\mathcal X,\mathcal F^{\mathcal X})$ is a standard Borel space then there exists a conditional probability distribution of $X$ given $Y$.
\end{theorem}






%%%%%%%%%% subsection
\subsection{A special version of $E(X|Y)$ using $\mathcal L_{X|Y=y}$.}


\begin{sectionassumption}
For the remainder of this section we continue with the following assumptions: Let  $(\Omega, \mathcal F, P)$ is  a probability space. Let $(\mathcal X, \mathcal F^{\mathcal X})$ and $(\mathcal Y, \mathcal F^{\mathcal Y})$ be two measurable spaces. Let $X\colon \Omega \rightarrow\mathcal X$ and $Y\colon \Omega\rightarrow \mathcal Y$ be two functions which are $\mcirc \mathcal F/\mathcal F^{\mathcal X}$ and $\mcirc \mathcal F/\mathcal F^{\mathcal Y}$ respectively. Let $\mathcal L_{X,Y}$ denote in induced measures $P(X,Y)^{-1}$ and $\mathcal L_Y$, $\mathcal L_X$ denote the marginal measures $PY^{-1}$ and $PX^{-1}$.
\end{sectionassumption}


Under the case that there exists a conditional probability distribution of $X$ given $Y$, $\mathcal L_{X|Y=y}$, and that $\mathcal X\subset \Bbb R$, we can define a special version of $E(X|Y)$ as follows
\[ E(X|Y=y) := \int_{\mathcal X} x \,d\mathcal L_{X|Y=y}(x). \]
The fact that this definition has the correct properties of a conditional expected value follows from the following Law of the Iterated integral which is essentially a Fubini-type theorem with more general joint probability measures on $\mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y}$.


\begin{theorem}[{\bf Law of the Iterated Integral, version 1}]
\label{thm: LIIv1}
Let $f(x,y)\colon \mathcal X\times \mathcal Y\rightarrow [0,\infty]$ be $\mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y}$-measurable. Then $\int_{\mathcal X} f(x,y)\,d\mathcal L_{X|Y=y}(x)$ is defined for all $y\in \mathcal Y$, a measurable function of $y\in \mathcal Y$ and is quasi-integrable with respect to $\mathcal L_Y$. Moreover,
\begin{equation}
\label{thm: LIIv1}
\int_{\mathcal X\times \mathcal Y} f d\mathcal L_{X,Y} =
\int_{\mathcal Y}\Bigl[\int_{\mathcal X} f(x,y)d\mathcal L_{X|Y=y}(x)\Bigr] d\mathcal L_Y(y).
\end{equation}
If $f$ is allowed to take negative values then (\ref{thm: LIIv1}) still holds provided $f\in Q(\mathcal L_{X,Y})$; in this case the inner integral on the right hand side of (\ref{thm: LIIv1}) is defined for $\mathcal L_Y$-a.e. $y$.
\end{theorem}


This theorem is a re-statement of the above LII, but is more specific as to how it allows us to define a special version of conditional expected value which can resolve the substitution fallacy.

\begin{theorem}[{\bf Law of the Iterated Integral, version 2}]
\label{thm: LII}
Let $f(x,y)\colon \mathcal X\times \mathcal Y\rightarrow \bar{\Bbb R}$ be $\mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y}$-measurable and quasi-integrable with respect to $\mathcal L_{X,Y}$. Define
\begin{equation}
\label{eq: subs principle}
h(y) := \int_{\mathcal X} f(x,y)\,d\mathcal L_{X|Y=y}(x).
\end{equation}
Then $h(y)$ is defined $\mathcal L_Y$-a.e., is $\mathcal F^{\mathcal Y}$-measurable and  $h(Y)$ is a version of $E\bigl(f(X,Y)|Y \bigr)$.
\end{theorem}


% The proof is left as an exercise.  However, lets parse what is means for $h(Y)$ to be a version of $E\bigl(F(X,Y)|Y \bigr)$. This means that $h(Y)$, as a function of $\omega\in \Omega$, is $\mcirc \sigma\langle Y\rangle/\mathcal B^{\bar{\Bbb R}}$ and $P$-quasi-integrable. Moreover, for any $B\in \sigma\langle Y\rangle$
% \[ \int_B F(X,Y)\, dP = \int_B h(Y)\, dP.   \]
% By Corollary \ref{cor: funs of measurable funs} we have that $h$ is $\mcirc \mathcal F^{\mathcal Y}/\mathcal B^{\bar{\Bbb R}}$. By change of variables (Theorem \ref{thm: change of variables}) we then have that $h(y)$ is also quasi-integrable with respect to $\mathcal L_Y := PY^{-1}$ and
% \[ \int_{A} F(x,y)\, d\mathcal L_{X,Y}(x,y) = \int_{A} h(y)\, d\mathcal L_{Y}(y)   \]
% for any $A\in \mathcal F^{\mathcal Y}$. Finally, in the special case that $A=\mathcal F^{\mathcal Y}$ or $B=\Omega$ one gets
% \begin{align*}
% E\bigl[ F(X,Y) \bigr]
% &= E\bigl[ \underbrace{E\bigl(F(X,Y) \bigl| Y\bigr)}_{h(Y)} \bigr] \\
% &= \int_{\mathcal Y} \underbrace{ E\bigl(F(X,y) \bigl| Y=y\bigr)}_{h(y)}\,dPY^{-1}.
% \end{align*}


Remember that $\mathcal L_{X|Y=y}$ is a probability distribution on $(\mathcal X,\mathcal F^{\mathcal X})$ for each $y\in \mathcal Y$. Therefore we can interpret $h(y)$ as the expected value of $f(X,y)$ where $X\sim \mathcal L_{X|y=y}$. In particular,
\[
h(y)=_{\tiny a.e.} E(f(X,y)|Y=y).
\]
Or another way to put it, we can {\em define} $E(f(X,y)|Y=y)$ to be $h(y)$ ($h$ being defined $\mathcal L_Y$-a.e.). Then under this definition the Law of the Iterated Integral tells us that  $h(Y)$ is a version of $E(f(X,Y)|Y)$ which resolves the substitution fallacy since
\[
E(f(X,Y)|Y=y) =_{\tiny a.e.} h(y) =_{\tiny a.e.} E(f(X,y)|Y=y)
\]

\begin{corollary}[{\bf Resolving the substitution fallacy}]
Let $f(x,y)\colon \mathcal X\times \mathcal Y\rightarrow \bar{\Bbb R}$ be $\mcirc \mathcal F^{\mathcal X}\otimes \mathcal F^{\mathcal Y}/\mathcal B^{\bar{\Bbb R}}$ and quasi integrable with respect to $\mathcal L_{X,Y}$.  If we define $E(f(X,y) |Y=y)$ to denote the function $h(y)$ as defined in (\ref{eq: subs principle}) then $E(f(X,y)|Y=y)$ is a version of $E(f(X,Y) |Y=y)$.
\end{corollary}




\begin{exercise} Prove the Law of the Iterated Integral for conditional probability distributions. Hint: Mimic the proof of Fubinito. Notice that, in the proof of Fubinito,  the key was the identity
\[P_1\otimes P_2(F) = \int_{\Omega_1} P_2(F_{\omega_1}) \, dP_1(\omega_1).   \]
For the proof LII notice that they analogous key formula is

\[\mathcal L_{X,Y}(F) = \int_{\mathcal Y} \mathcal L_{X|Y=y}(F_y) \,d\mathcal L_Y(y) \]
which we already proved in the notes.
\end{exercise}

For the following two exercises suppose $\mathcal X$  and $\mathcal Y$ are metric spaces. Let $ \mathcal F^{\mathcal X}$ and $\mathcal F^{\mathcal Y}$ be the Borel $\sigma$-fields generated by the respective metrics.  Let $X\colon \Omega \rightarrow\mathcal X$ and $Y\colon \Omega\rightarrow \mathcal Y$ be two functions which are $\mcirc \mathcal F/\mathcal F^{\mathcal X}$ and $\mcirc \mathcal F/\mathcal F^{\mathcal Y}$ respectively.

\begin{definition}
A conditional probability distribution $\mathcal L_{X|Y=y}$ is said to be  {\bf weakly continuous} if for every $y\in \mathcal Y$
\[
\mathcal L_{X|Y=y_n} \rightsquigarrow  \mathcal L_{X|Y=y}
\]
whenever $y_n\rightarrow y$.
\end{definition}


\begin{exercise}
\label{continuous uniqueness}
Show that if $P(Y\in G)>0$ for each non-empty open $G\subset \mathcal Y$ then any weakly continuous conditional probability distribution (cpd) for $X$ given $Y$ is completely unique. In particular, show that if $\mathcal L_{X|Y=y}$ and  $\mathcal L^*_{X|Y=y}$ are two weakly continuous cpds then $\mathcal L_{X|Y=y}=\mathcal L^*_{X|Y=y}$ for all $y\in \mathcal Y$.

Hint: Show that whenever $f\colon \mathcal X\rightarrow \Bbb R$ is continuous and bounded then $\int_{\mathcal X} f d\mathcal L_{X|Y=y} - \int_{\mathcal X} f d\mathcal L^*_{X|Y=y}$
is zero for all $y\in \mathcal Y$.
\end{exercise}

\begin{exercise}
\label{limiting construction}
Suppose $P(Y\in G)>0$ for each non-empty open $G\subset \mathcal Y$. Let $B_{y_0}^\epsilon := \{y\in\mathcal Y\colon d_Y(y,y_0)<\epsilon \}$ and  define
\[
\mathcal L_{X|Y\in B_{y_0}^\epsilon}(\bullet):=\frac{P[X\in \bullet, Y\in B_{y_0}^\epsilon]}{P[Y\in B_{y_0}^\epsilon]}
\]
which is a probability measure on $(\mathcal X, \mathcal F^{\mathcal X})$.
Show that if there exists a weakly continuous cpd  $\mathcal L_{X|Y=y}$  then for each $y_0\in \mathcal Y$
\[
\mathcal L_{X|Y\in B_{y_0}^\epsilon} \rightsquigarrow  \mathcal L_{X|Y=y_0}
\]
 as $\epsilon\rightarrow 0$.


Hint: When $f\colon \mathcal X\rightarrow \Bbb R$ is continuous and bounded show that
\begin{align*}
E\bigl(f| Y\in B_{y_0}^\epsilon\bigr) &= \frac{E\bigl[f(X)I_{B_{y_0}^\epsilon}(Y)\bigr]}{P\bigl[Y\in B_{y_0}^\epsilon\bigr]} = \int_{B_{y_0}^\epsilon}\frac{E\bigl(f|Y=y \bigr)}{P\bigl[Y\in B_{y_0}^\epsilon\bigr]}  dPY^{-1}(y).
\end{align*}
Be sure to be precise about what $E\bigl(f|Y=y \bigr)$ denotes.
\end{exercise}



\subsection{Application: $L_2$ Wasserstein metric, optimal coupling, disintegration}
